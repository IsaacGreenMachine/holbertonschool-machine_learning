{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari,accept-rom-license]\n",
        "# from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import gym\n",
        "from collections import deque\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "_toM-ns4UxPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92f1d03-78f0-4be9-9254-91063a631b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.13.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=dbbd1e2753aca25d7775bbd56d96d5559039f4976d9cb64b73dd02f32dda08ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHrZAVk9UdcJ"
      },
      "outputs": [],
      "source": [
        "best_score_filepath = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning/reinforcement learning/0x01 - deep_q_learning/bestScore.txt\"\n",
        "model_filepath = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning/reinforcement learning/0x01 - deep_q_learning/workingPolicyWeights.h5\"\n",
        "movie_save_path = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning/reinforcement learning/0x01 - deep_q_learning/playmovie.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrkN4VS3VxFT"
      },
      "source": [
        "## Train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3szZ9sbrfKW"
      },
      "outputs": [],
      "source": [
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            # noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "            noops = np.random.randint(1, self.noop_max + 1) \n",
        "\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class TimeLimit(gym.Wrapper):\n",
        "    def __init__(self, env, max_episode_steps=None):\n",
        "        super(TimeLimit, self).__init__(env)\n",
        "        self._max_episode_steps = max_episode_steps\n",
        "        self._elapsed_steps = 0\n",
        "\n",
        "    def step(self, ac):\n",
        "        observation, reward, done, info = self.env.step(ac)\n",
        "        self._elapsed_steps += 1\n",
        "        if self._elapsed_steps >= self._max_episode_steps:\n",
        "            done = True\n",
        "            info['TimeLimit.truncated'] = True\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self._elapsed_steps = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "  \n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
        "        \"\"\"\n",
        "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which\n",
        "        observation should be warped.\n",
        "        \"\"\"\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "    def observation(self, obs):\n",
        "        if self._key is None:\n",
        "            frame = obs\n",
        "        else:\n",
        "            frame = obs[self._key]\n",
        "\n",
        "        if self._grayscale:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        if self._grayscale:\n",
        "            frame = np.expand_dims(frame, -1)\n",
        "\n",
        "        if self._key is None:\n",
        "            obs = frame\n",
        "        else:\n",
        "            obs = obs.copy()\n",
        "            obs[self._key] = frame\n",
        "        return obs\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t29gil3grMkh"
      },
      "outputs": [],
      "source": [
        "def make_atari(env_id, max_episode_steps=None):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if max_episode_steps is not None:\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    # if clip_rewards:\n",
        "    #     env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env\n",
        "\n",
        "def create_q_model(num_actions=4):\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_WvBgLEEqlq",
        "outputId": "2ca1f449-1984-4a1d-a06c-aa5f870cbef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3444837047, 2669555309)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 32  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# Use the Baseline Atari environment because of Deepmind helper functions\n",
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
        "\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "env.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(env.reset()).shape)\n",
        "print(np.array(env.reset())[30, 30] )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffPDhTskvof-",
        "outputId": "41cceea5-0a54-40ab-830b-25d502ebb792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(84, 84, 4)\n",
            "[0.5803922 0.5803922 0.5803922 0.5803922]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9UAx-4bq0gT"
      },
      "outputs": [],
      "source": [
        "num_actions = 4\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = create_q_model()\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = create_q_model()\n",
        "\n",
        "\n",
        "'''\n",
        "model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Machine Learning/reinforcement learning/0x01 - deep_q_learning/workingPolicy.h5')\n",
        "\n",
        "model_target = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Machine Learning/reinforcement learning/0x01 - deep_q_learning/workingPolicy.h5')\n",
        "'''\n",
        "\n",
        "model.load_weights(model_filepath)\n",
        "model_target.load_weights(model_filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wETDM2BLq4Fs",
        "outputId": "0e2b667b-47c1-4f28-b9ba-50b8855d888a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 0.38 at episode 138, frame count 5000\n",
            "running reward: 0.21 at episode 297, frame count 10000\n",
            "running reward: 0.29 at episode 447, frame count 15000\n",
            "running reward: 0.17 at episode 616, frame count 20000\n",
            "running reward: 0.24 at episode 765, frame count 25000\n",
            "running reward: 0.34 at episode 905, frame count 30000\n",
            "running reward: 0.26 at episode 1051, frame count 35000\n",
            "running reward: 0.25 at episode 1209, frame count 40000\n",
            "running reward: 0.27 at episode 1354, frame count 45000\n",
            "running reward: 0.32 at episode 1502, frame count 50000\n",
            "running reward: 0.41 at episode 1627, frame count 55000\n",
            "running reward: 0.30 at episode 1769, frame count 60000\n",
            "running reward: 0.31 at episode 1901, frame count 65000\n",
            "running reward: 0.38 at episode 2034, frame count 70000\n",
            "running reward: 0.37 at episode 2175, frame count 75000\n",
            "running reward: 0.31 at episode 2313, frame count 80000\n",
            "running reward: 0.30 at episode 2466, frame count 85000\n",
            "running reward: 0.25 at episode 2618, frame count 90000\n",
            "running reward: 0.27 at episode 2764, frame count 95000\n",
            "running reward: 0.41 at episode 2895, frame count 100000\n",
            "running reward: 0.14 at episode 3060, frame count 105000\n",
            "running reward: 0.33 at episode 3195, frame count 110000\n",
            "running reward: 0.23 at episode 3351, frame count 115000\n",
            "running reward: 0.32 at episode 3490, frame count 120000\n",
            "running reward: 0.32 at episode 3640, frame count 125000\n",
            "running reward: 0.44 at episode 3768, frame count 130000\n",
            "running reward: 0.32 at episode 3909, frame count 135000\n",
            "running reward: 0.44 at episode 4034, frame count 140000\n",
            "running reward: 0.34 at episode 4177, frame count 145000\n",
            "running reward: 0.35 at episode 4317, frame count 150000\n",
            "running reward: 0.38 at episode 4444, frame count 155000\n",
            "running reward: 0.53 at episode 4569, frame count 160000\n",
            "running reward: 0.44 at episode 4694, frame count 165000\n",
            "running reward: 0.42 at episode 4822, frame count 170000\n",
            "running reward: 0.30 at episode 4961, frame count 175000\n",
            "running reward: 0.51 at episode 5079, frame count 180000\n",
            "running reward: 0.27 at episode 5230, frame count 185000\n",
            "running reward: 0.28 at episode 5368, frame count 190000\n",
            "running reward: 0.36 at episode 5503, frame count 195000\n",
            "running reward: 0.39 at episode 5633, frame count 200000\n",
            "running reward: 0.65 at episode 5734, frame count 205000\n",
            "running reward: 0.33 at episode 5877, frame count 210000\n",
            "running reward: 0.35 at episode 6014, frame count 215000\n",
            "running reward: 0.35 at episode 6151, frame count 220000\n",
            "running reward: 0.44 at episode 6276, frame count 225000\n",
            "running reward: 0.39 at episode 6414, frame count 230000\n",
            "running reward: 0.34 at episode 6547, frame count 235000\n",
            "running reward: 0.54 at episode 6665, frame count 240000\n",
            "running reward: 0.58 at episode 6771, frame count 245000\n",
            "running reward: 0.36 at episode 6912, frame count 250000\n",
            "running reward: 0.47 at episode 7031, frame count 255000\n",
            "running reward: 0.53 at episode 7147, frame count 260000\n",
            "running reward: 0.78 at episode 7241, frame count 265000\n",
            "running reward: 0.54 at episode 7354, frame count 270000\n",
            "running reward: 0.49 at episode 7476, frame count 275000\n",
            "running reward: 0.58 at episode 7584, frame count 280000\n",
            "running reward: 0.44 at episode 7706, frame count 285000\n",
            "running reward: 0.50 at episode 7821, frame count 290000\n",
            "running reward: 0.47 at episode 7938, frame count 295000\n",
            "running reward: 0.52 at episode 8051, frame count 300000\n",
            "running reward: 0.40 at episode 8181, frame count 305000\n",
            "running reward: 0.57 at episode 8296, frame count 310000\n",
            "running reward: 0.50 at episode 8412, frame count 315000\n",
            "running reward: 0.37 at episode 8548, frame count 320000\n",
            "running reward: 0.42 at episode 8678, frame count 325000\n",
            "running reward: 0.33 at episode 8823, frame count 330000\n",
            "running reward: 0.56 at episode 8938, frame count 335000\n",
            "running reward: 0.42 at episode 9065, frame count 340000\n",
            "running reward: 0.36 at episode 9185, frame count 345000\n",
            "running reward: 0.45 at episode 9308, frame count 350000\n",
            "running reward: 0.44 at episode 9428, frame count 355000\n",
            "running reward: 0.47 at episode 9558, frame count 360000\n",
            "running reward: 0.72 at episode 9657, frame count 365000\n",
            "running reward: 0.46 at episode 9779, frame count 370000\n",
            "running reward: 0.58 at episode 9891, frame count 375000\n",
            "running reward: 0.67 at episode 9991, frame count 380000\n",
            "running reward: 0.60 at episode 10097, frame count 385000\n",
            "running reward: 0.74 at episode 10193, frame count 390000\n",
            "running reward: 0.71 at episode 10291, frame count 395000\n",
            "running reward: 0.52 at episode 10409, frame count 400000\n",
            "running reward: 0.66 at episode 10512, frame count 405000\n",
            "running reward: 0.64 at episode 10616, frame count 410000\n",
            "running reward: 0.77 at episode 10716, frame count 415000\n",
            "running reward: 0.60 at episode 10824, frame count 420000\n",
            "running reward: 0.54 at episode 10938, frame count 425000\n",
            "running reward: 0.52 at episode 11059, frame count 430000\n",
            "running reward: 0.56 at episode 11174, frame count 435000\n",
            "running reward: 0.60 at episode 11286, frame count 440000\n",
            "running reward: 0.66 at episode 11395, frame count 445000\n",
            "running reward: 0.55 at episode 11507, frame count 450000\n",
            "running reward: 0.75 at episode 11604, frame count 455000\n",
            "running reward: 0.60 at episode 11709, frame count 460000\n",
            "running reward: 0.73 at episode 11814, frame count 465000\n",
            "running reward: 0.51 at episode 11934, frame count 470000\n",
            "running reward: 0.73 at episode 12033, frame count 475000\n",
            "running reward: 0.57 at episode 12146, frame count 480000\n",
            "running reward: 1.10 at episode 12222, frame count 485000\n",
            "running reward: 1.17 at episode 12306, frame count 490000\n",
            "running reward: 0.80 at episode 12400, frame count 495000\n",
            "running reward: 1.11 at episode 12478, frame count 500000\n",
            "running reward: 1.22 at episode 12557, frame count 505000\n",
            "running reward: 0.91 at episode 12650, frame count 510000\n",
            "running reward: 1.04 at episode 12728, frame count 515000\n",
            "running reward: 1.16 at episode 12808, frame count 520000\n",
            "running reward: 1.12 at episode 12886, frame count 525000\n",
            "running reward: 1.07 at episode 12967, frame count 530000\n",
            "running reward: 1.05 at episode 13054, frame count 535000\n",
            "running reward: 1.28 at episode 13130, frame count 540000\n",
            "running reward: 1.59 at episode 13201, frame count 545000\n",
            "running reward: 1.65 at episode 13267, frame count 550000\n",
            "running reward: 1.25 at episode 13345, frame count 555000\n",
            "running reward: 1.42 at episode 13412, frame count 560000\n",
            "running reward: 1.48 at episode 13483, frame count 565000\n",
            "running reward: 1.34 at episode 13557, frame count 570000\n",
            "running reward: 1.50 at episode 13619, frame count 575000\n",
            "running reward: 1.77 at episode 13683, frame count 580000\n",
            "running reward: 1.44 at episode 13763, frame count 585000\n",
            "running reward: 0.93 at episode 13852, frame count 590000\n",
            "running reward: 1.09 at episode 13931, frame count 595000\n",
            "running reward: 1.37 at episode 13999, frame count 600000\n",
            "running reward: 1.43 at episode 14067, frame count 605000\n",
            "running reward: 1.63 at episode 14121, frame count 610000\n",
            "running reward: 2.21 at episode 14175, frame count 615000\n",
            "running reward: 1.81 at episode 14240, frame count 620000\n",
            "running reward: 1.50 at episode 14310, frame count 625000\n",
            "running reward: 1.85 at episode 14369, frame count 630000\n",
            "running reward: 1.81 at episode 14432, frame count 635000\n",
            "running reward: 2.40 at episode 14478, frame count 640000\n",
            "running reward: 2.54 at episode 14534, frame count 645000\n",
            "running reward: 2.21 at episode 14590, frame count 650000\n",
            "running reward: 2.09 at episode 14644, frame count 655000\n",
            "running reward: 1.95 at episode 14711, frame count 660000\n",
            "running reward: 1.64 at episode 14767, frame count 665000\n",
            "running reward: 1.56 at episode 14835, frame count 670000\n",
            "running reward: 2.04 at episode 14891, frame count 675000\n",
            "running reward: 2.25 at episode 14948, frame count 680000\n",
            "running reward: 2.08 at episode 15007, frame count 685000\n",
            "running reward: 1.44 at episode 15080, frame count 690000\n",
            "running reward: 1.71 at episode 15139, frame count 695000\n",
            "running reward: 2.29 at episode 15191, frame count 700000\n",
            "running reward: 2.55 at episode 15242, frame count 705000\n",
            "running reward: 1.68 at episode 15312, frame count 710000\n",
            "running reward: 1.89 at episode 15364, frame count 715000\n",
            "running reward: 2.34 at episode 15418, frame count 720000\n",
            "running reward: 2.27 at episode 15473, frame count 725000\n",
            "running reward: 2.48 at episode 15523, frame count 730000\n",
            "running reward: 3.00 at episode 15571, frame count 735000\n",
            "running reward: 2.28 at episode 15630, frame count 740000\n",
            "running reward: 2.03 at episode 15693, frame count 745000\n",
            "running reward: 2.19 at episode 15748, frame count 750000\n",
            "running reward: 2.42 at episode 15796, frame count 755000\n",
            "running reward: 2.24 at episode 15855, frame count 760000\n",
            "running reward: 2.47 at episode 15898, frame count 765000\n",
            "running reward: 3.34 at episode 15937, frame count 770000\n",
            "running reward: 2.98 at episode 15994, frame count 775000\n",
            "running reward: 2.39 at episode 16043, frame count 780000\n",
            "running reward: 2.70 at episode 16086, frame count 785000\n",
            "running reward: 3.34 at episode 16128, frame count 790000\n",
            "running reward: 3.35 at episode 16177, frame count 795000\n",
            "running reward: 3.46 at episode 16213, frame count 800000\n",
            "running reward: 3.25 at episode 16256, frame count 805000\n",
            "running reward: 3.18 at episode 16301, frame count 810000\n",
            "running reward: 3.20 at episode 16344, frame count 815000\n",
            "running reward: 3.83 at episode 16379, frame count 820000\n",
            "running reward: 4.09 at episode 16421, frame count 825000\n",
            "running reward: 3.60 at episode 16459, frame count 830000\n",
            "running reward: 3.97 at episode 16497, frame count 835000\n",
            "running reward: 4.60 at episode 16531, frame count 840000\n",
            "running reward: 6.84 at episode 16571, frame count 845000\n",
            "running reward: 7.11 at episode 16599, frame count 850000\n",
            "running reward: 7.13 at episode 16632, frame count 855000\n",
            "running reward: 5.46 at episode 16661, frame count 860000\n",
            "running reward: 5.71 at episode 16690, frame count 865000\n",
            "running reward: 5.33 at episode 16725, frame count 870000\n",
            "running reward: 5.42 at episode 16759, frame count 875000\n",
            "running reward: 6.08 at episode 16785, frame count 880000\n",
            "running reward: 6.72 at episode 16807, frame count 885000\n",
            "running reward: 6.76 at episode 16840, frame count 890000\n",
            "running reward: 7.48 at episode 16867, frame count 895000\n",
            "running reward: 7.32 at episode 16892, frame count 900000\n",
            "running reward: 7.97 at episode 16920, frame count 905000\n",
            "running reward: 8.67 at episode 16941, frame count 910000\n",
            "running reward: 8.63 at episode 16969, frame count 915000\n",
            "running reward: 7.71 at episode 17004, frame count 920000\n",
            "running reward: 7.69 at episode 17029, frame count 925000\n",
            "running reward: 7.41 at episode 17051, frame count 930000\n",
            "running reward: 7.77 at episode 17080, frame count 935000\n",
            "running reward: 8.18 at episode 17108, frame count 940000\n",
            "running reward: 7.76 at episode 17135, frame count 945000\n",
            "running reward: 7.94 at episode 17155, frame count 950000\n",
            "running reward: 9.01 at episode 17177, frame count 955000\n",
            "running reward: 9.32 at episode 17196, frame count 960000\n",
            "running reward: 10.83 at episode 17218, frame count 965000\n",
            "running reward: 10.45 at episode 17248, frame count 970000\n",
            "running reward: 9.18 at episode 17278, frame count 975000\n",
            "running reward: 8.91 at episode 17302, frame count 980000\n",
            "running reward: 8.61 at episode 17324, frame count 985000\n",
            "running reward: 9.40 at episode 17350, frame count 990000\n",
            "running reward: 13.49 at episode 17373, frame count 995000\n",
            "running reward: 13.44 at episode 17400, frame count 1000000\n",
            "running reward: 13.09 at episode 17424, frame count 1005000\n",
            "running reward: 12.78 at episode 17448, frame count 1010000\n",
            "running reward: 9.71 at episode 17470, frame count 1015000\n",
            "running reward: 11.70 at episode 17491, frame count 1020000\n",
            "running reward: 12.22 at episode 17512, frame count 1025000\n",
            "running reward: 13.34 at episode 17533, frame count 1030000\n",
            "running reward: 13.96 at episode 17553, frame count 1035000\n",
            "new best reward: 14.75 saving model\n",
            "new best reward: 14.77 saving model\n",
            "new best reward: 16.88 saving model\n",
            "new best reward: 16.92 saving model\n",
            "new best reward: 16.93 saving model\n",
            "new best reward: 17.04 saving model\n",
            "new best reward: 17.09 saving model\n",
            "new best reward: 17.13 saving model\n",
            "running reward: 14.99 at episode 17575, frame count 1040000\n",
            "new best reward: 17.89 saving model\n",
            "running reward: 17.22 at episode 17597, frame count 1045000\n",
            "new best reward: 17.91 saving model\n",
            "new best reward: 17.94 saving model\n",
            "running reward: 16.85 at episode 17624, frame count 1050000\n",
            "running reward: 16.09 at episode 17647, frame count 1055000\n",
            "running reward: 12.80 at episode 17666, frame count 1060000\n",
            "running reward: 13.83 at episode 17690, frame count 1065000\n",
            "running reward: 13.58 at episode 17711, frame count 1070000\n",
            "running reward: 14.17 at episode 17734, frame count 1075000\n",
            "running reward: 15.57 at episode 17754, frame count 1080000\n",
            "running reward: 12.71 at episode 17771, frame count 1085000\n",
            "running reward: 12.62 at episode 17796, frame count 1090000\n",
            "running reward: 14.01 at episode 17814, frame count 1095000\n",
            "running reward: 14.57 at episode 17837, frame count 1100000\n",
            "running reward: 14.84 at episode 17856, frame count 1105000\n",
            "running reward: 15.34 at episode 17875, frame count 1110000\n",
            "new best reward: 17.99 saving model\n",
            "new best reward: 18.08 saving model\n",
            "new best reward: 18.13 saving model\n",
            "new best reward: 18.19 saving model\n",
            "running reward: 17.93 at episode 17892, frame count 1115000\n",
            "new best reward: 18.23 saving model\n",
            "new best reward: 18.33 saving model\n",
            "new best reward: 18.45 saving model\n",
            "new best reward: 18.54 saving model\n",
            "new best reward: 18.55 saving model\n",
            "new best reward: 18.63 saving model\n",
            "running reward: 17.55 at episode 17911, frame count 1120000\n",
            "running reward: 17.16 at episode 17930, frame count 1125000\n",
            "running reward: 16.83 at episode 17951, frame count 1130000\n",
            "running reward: 16.28 at episode 17972, frame count 1135000\n",
            "running reward: 15.93 at episode 17993, frame count 1140000\n",
            "running reward: 14.88 at episode 18016, frame count 1145000\n",
            "running reward: 14.25 at episode 18039, frame count 1150000\n",
            "running reward: 15.13 at episode 18057, frame count 1155000\n",
            "running reward: 15.89 at episode 18078, frame count 1160000\n",
            "running reward: 16.40 at episode 18097, frame count 1165000\n",
            "running reward: 16.93 at episode 18115, frame count 1170000\n",
            "running reward: 16.37 at episode 18140, frame count 1175000\n",
            "running reward: 16.11 at episode 18157, frame count 1180000\n",
            "running reward: 15.85 at episode 18180, frame count 1185000\n",
            "running reward: 14.36 at episode 18197, frame count 1190000\n",
            "running reward: 15.73 at episode 18216, frame count 1195000\n",
            "running reward: 18.37 at episode 18236, frame count 1200000\n",
            "running reward: 18.59 at episode 18256, frame count 1205000\n",
            "new best reward: 19.43 saving model\n",
            "new best reward: 19.5 saving model\n",
            "running reward: 18.37 at episode 18278, frame count 1210000\n",
            "new best reward: 21.4 saving model\n",
            "new best reward: 22.5 saving model\n",
            "new best reward: 22.65 saving model\n",
            "running reward: 22.51 at episode 18297, frame count 1215000\n",
            "new best reward: 22.94 saving model\n",
            "running reward: 22.34 at episode 18317, frame count 1220000\n",
            "new best reward: 23.0 saving model\n",
            "new best reward: 23.1 saving model\n",
            "running reward: 22.61 at episode 18337, frame count 1225000\n",
            "new best reward: 23.57 saving model\n",
            "new best reward: 23.8 saving model\n",
            "new best reward: 24.03 saving model\n",
            "new best reward: 24.05 saving model\n",
            "new best reward: 24.22 saving model\n",
            "new best reward: 24.87 saving model\n",
            "running reward: 24.54 at episode 18357, frame count 1230000\n",
            "new best reward: 24.93 saving model\n",
            "new best reward: 25.42 saving model\n",
            "running reward: 24.35 at episode 18375, frame count 1235000\n",
            "running reward: 19.63 at episode 18397, frame count 1240000\n",
            "running reward: 17.64 at episode 18417, frame count 1245000\n",
            "running reward: 15.73 at episode 18438, frame count 1250000\n",
            "running reward: 13.11 at episode 18460, frame count 1255000\n",
            "running reward: 13.66 at episode 18483, frame count 1260000\n",
            "running reward: 13.94 at episode 18502, frame count 1265000\n",
            "running reward: 14.30 at episode 18521, frame count 1270000\n",
            "running reward: 14.40 at episode 18540, frame count 1275000\n",
            "running reward: 17.31 at episode 18559, frame count 1280000\n",
            "running reward: 15.61 at episode 18579, frame count 1285000\n",
            "running reward: 17.68 at episode 18598, frame count 1290000\n",
            "running reward: 16.81 at episode 18617, frame count 1295000\n",
            "running reward: 17.59 at episode 18634, frame count 1300000\n",
            "running reward: 14.57 at episode 18654, frame count 1305000\n",
            "running reward: 15.37 at episode 18671, frame count 1310000\n",
            "running reward: 13.14 at episode 18698, frame count 1315000\n",
            "running reward: 12.53 at episode 18721, frame count 1320000\n",
            "running reward: 12.27 at episode 18740, frame count 1325000\n",
            "running reward: 12.61 at episode 18761, frame count 1330000\n",
            "running reward: 12.08 at episode 18785, frame count 1335000\n",
            "running reward: 12.32 at episode 18808, frame count 1340000\n",
            "running reward: 12.81 at episode 18826, frame count 1345000\n",
            "running reward: 13.83 at episode 18843, frame count 1350000\n",
            "running reward: 14.67 at episode 18866, frame count 1355000\n",
            "running reward: 16.49 at episode 18880, frame count 1360000\n",
            "running reward: 16.17 at episode 18900, frame count 1365000\n",
            "running reward: 15.47 at episode 18930, frame count 1370000\n",
            "running reward: 15.44 at episode 18948, frame count 1375000\n",
            "running reward: 14.88 at episode 18970, frame count 1380000\n",
            "running reward: 13.77 at episode 18991, frame count 1385000\n",
            "running reward: 12.88 at episode 19013, frame count 1390000\n",
            "running reward: 10.95 at episode 19037, frame count 1395000\n",
            "running reward: 11.20 at episode 19058, frame count 1400000\n",
            "running reward: 15.06 at episode 19076, frame count 1405000\n",
            "running reward: 17.57 at episode 19096, frame count 1410000\n",
            "running reward: 19.46 at episode 19115, frame count 1415000\n",
            "running reward: 20.94 at episode 19134, frame count 1420000\n",
            "running reward: 22.52 at episode 19153, frame count 1425000\n",
            "running reward: 18.87 at episode 19180, frame count 1430000\n",
            "running reward: 16.66 at episode 19200, frame count 1435000\n",
            "running reward: 17.30 at episode 19217, frame count 1440000\n",
            "running reward: 15.33 at episode 19244, frame count 1445000\n",
            "running reward: 17.75 at episode 19261, frame count 1450000\n",
            "running reward: 19.37 at episode 19276, frame count 1455000\n",
            "running reward: 20.07 at episode 19301, frame count 1460000\n",
            "running reward: 18.16 at episode 19322, frame count 1465000\n",
            "running reward: 18.54 at episode 19345, frame count 1470000\n",
            "running reward: 15.99 at episode 19363, frame count 1475000\n",
            "running reward: 14.39 at episode 19381, frame count 1480000\n",
            "running reward: 13.08 at episode 19405, frame count 1485000\n",
            "running reward: 11.63 at episode 19427, frame count 1490000\n",
            "running reward: 11.92 at episode 19450, frame count 1495000\n",
            "running reward: 11.04 at episode 19470, frame count 1500000\n",
            "running reward: 12.75 at episode 19491, frame count 1505000\n",
            "running reward: 13.82 at episode 19512, frame count 1510000\n",
            "running reward: 15.41 at episode 19531, frame count 1515000\n",
            "running reward: 15.57 at episode 19551, frame count 1520000\n",
            "running reward: 12.72 at episode 19581, frame count 1525000\n",
            "running reward: 12.90 at episode 19596, frame count 1530000\n",
            "running reward: 12.32 at episode 19621, frame count 1535000\n",
            "running reward: 13.50 at episode 19640, frame count 1540000\n",
            "running reward: 13.88 at episode 19665, frame count 1545000\n",
            "running reward: 14.60 at episode 19684, frame count 1550000\n",
            "running reward: 14.25 at episode 19708, frame count 1555000\n",
            "running reward: 14.35 at episode 19730, frame count 1560000\n",
            "running reward: 13.48 at episode 19753, frame count 1565000\n",
            "running reward: 14.89 at episode 19775, frame count 1570000\n",
            "running reward: 15.76 at episode 19792, frame count 1575000\n",
            "running reward: 16.46 at episode 19816, frame count 1580000\n",
            "running reward: 16.62 at episode 19837, frame count 1585000\n",
            "running reward: 14.05 at episode 19861, frame count 1590000\n",
            "running reward: 14.88 at episode 19881, frame count 1595000\n",
            "running reward: 14.84 at episode 19906, frame count 1600000\n",
            "running reward: 14.67 at episode 19926, frame count 1605000\n",
            "running reward: 18.81 at episode 19950, frame count 1610000\n",
            "running reward: 18.90 at episode 19970, frame count 1615000\n",
            "running reward: 19.65 at episode 19992, frame count 1620000\n",
            "running reward: 18.97 at episode 20010, frame count 1625000\n",
            "running reward: 16.93 at episode 20028, frame count 1630000\n",
            "running reward: 15.16 at episode 20046, frame count 1635000\n",
            "running reward: 13.50 at episode 20076, frame count 1640000\n",
            "running reward: 12.28 at episode 20097, frame count 1645000\n",
            "running reward: 12.98 at episode 20116, frame count 1650000\n",
            "running reward: 13.47 at episode 20133, frame count 1655000\n",
            "running reward: 12.97 at episode 20155, frame count 1660000\n",
            "running reward: 14.80 at episode 20174, frame count 1665000\n",
            "running reward: 16.11 at episode 20192, frame count 1670000\n",
            "running reward: 15.68 at episode 20215, frame count 1675000\n",
            "running reward: 16.83 at episode 20234, frame count 1680000\n",
            "running reward: 16.98 at episode 20258, frame count 1685000\n",
            "running reward: 16.82 at episode 20276, frame count 1690000\n",
            "running reward: 18.28 at episode 20297, frame count 1695000\n",
            "running reward: 18.50 at episode 20320, frame count 1700000\n",
            "running reward: 18.31 at episode 20340, frame count 1705000\n",
            "running reward: 17.14 at episode 20365, frame count 1710000\n",
            "running reward: 13.71 at episode 20390, frame count 1715000\n",
            "running reward: 16.85 at episode 20410, frame count 1720000\n",
            "running reward: 15.13 at episode 20429, frame count 1725000\n",
            "running reward: 15.16 at episode 20446, frame count 1730000\n",
            "running reward: 18.01 at episode 20472, frame count 1735000\n",
            "running reward: 20.84 at episode 20492, frame count 1740000\n",
            "running reward: 17.16 at episode 20516, frame count 1745000\n",
            "running reward: 17.80 at episode 20536, frame count 1750000\n",
            "running reward: 15.59 at episode 20563, frame count 1755000\n",
            "running reward: 14.56 at episode 20588, frame count 1760000\n",
            "running reward: 13.59 at episode 20610, frame count 1765000\n",
            "running reward: 13.16 at episode 20630, frame count 1770000\n",
            "running reward: 14.25 at episode 20652, frame count 1775000\n",
            "running reward: 12.59 at episode 20671, frame count 1780000\n",
            "running reward: 13.99 at episode 20692, frame count 1785000\n",
            "running reward: 13.58 at episode 20712, frame count 1790000\n",
            "running reward: 16.32 at episode 20733, frame count 1795000\n",
            "running reward: 15.57 at episode 20757, frame count 1800000\n",
            "running reward: 15.90 at episode 20778, frame count 1805000\n",
            "running reward: 15.54 at episode 20802, frame count 1810000\n",
            "running reward: 13.50 at episode 20820, frame count 1815000\n",
            "running reward: 14.40 at episode 20839, frame count 1820000\n",
            "running reward: 16.58 at episode 20859, frame count 1825000\n",
            "running reward: 15.41 at episode 20886, frame count 1830000\n",
            "running reward: 13.55 at episode 20910, frame count 1835000\n",
            "running reward: 14.00 at episode 20930, frame count 1840000\n",
            "running reward: 15.99 at episode 20947, frame count 1845000\n",
            "running reward: 14.24 at episode 20967, frame count 1850000\n",
            "running reward: 17.21 at episode 20989, frame count 1855000\n",
            "running reward: 20.19 at episode 21006, frame count 1860000\n",
            "running reward: 18.71 at episode 21029, frame count 1865000\n",
            "running reward: 16.25 at episode 21049, frame count 1870000\n",
            "running reward: 14.47 at episode 21073, frame count 1875000\n",
            "running reward: 12.17 at episode 21096, frame count 1880000\n",
            "running reward: 11.26 at episode 21121, frame count 1885000\n",
            "running reward: 15.04 at episode 21141, frame count 1890000\n",
            "running reward: 15.00 at episode 21160, frame count 1895000\n",
            "running reward: 16.05 at episode 21182, frame count 1900000\n",
            "running reward: 16.87 at episode 21205, frame count 1905000\n",
            "running reward: 19.30 at episode 21222, frame count 1910000\n",
            "running reward: 16.08 at episode 21246, frame count 1915000\n",
            "running reward: 15.93 at episode 21266, frame count 1920000\n",
            "running reward: 16.28 at episode 21290, frame count 1925000\n",
            "running reward: 13.95 at episode 21309, frame count 1930000\n",
            "running reward: 14.63 at episode 21328, frame count 1935000\n",
            "running reward: 13.28 at episode 21349, frame count 1940000\n",
            "running reward: 14.77 at episode 21369, frame count 1945000\n",
            "running reward: 15.37 at episode 21390, frame count 1950000\n",
            "running reward: 15.90 at episode 21414, frame count 1955000\n",
            "running reward: 15.18 at episode 21436, frame count 1960000\n",
            "running reward: 15.36 at episode 21454, frame count 1965000\n",
            "running reward: 12.72 at episode 21480, frame count 1970000\n",
            "running reward: 11.45 at episode 21501, frame count 1975000\n",
            "running reward: 11.81 at episode 21522, frame count 1980000\n",
            "running reward: 12.53 at episode 21541, frame count 1985000\n",
            "running reward: 12.44 at episode 21561, frame count 1990000\n",
            "running reward: 13.55 at episode 21582, frame count 1995000\n",
            "running reward: 13.66 at episode 21603, frame count 2000000\n",
            "running reward: 13.85 at episode 21622, frame count 2005000\n",
            "running reward: 15.86 at episode 21641, frame count 2010000\n",
            "running reward: 15.89 at episode 21662, frame count 2015000\n",
            "running reward: 17.70 at episode 21679, frame count 2020000\n",
            "running reward: 17.34 at episode 21701, frame count 2025000\n",
            "running reward: 16.74 at episode 21723, frame count 2030000\n",
            "running reward: 14.95 at episode 21746, frame count 2035000\n",
            "running reward: 17.18 at episode 21764, frame count 2040000\n",
            "running reward: 19.12 at episode 21789, frame count 2045000\n",
            "running reward: 19.74 at episode 21808, frame count 2050000\n",
            "running reward: 21.68 at episode 21829, frame count 2055000\n",
            "running reward: 18.80 at episode 21854, frame count 2060000\n",
            "running reward: 18.91 at episode 21874, frame count 2065000\n",
            "running reward: 16.69 at episode 21895, frame count 2070000\n",
            "running reward: 15.70 at episode 21912, frame count 2075000\n",
            "running reward: 14.55 at episode 21933, frame count 2080000\n",
            "running reward: 18.08 at episode 21953, frame count 2085000\n",
            "running reward: 18.50 at episode 21977, frame count 2090000\n",
            "running reward: 17.26 at episode 21998, frame count 2095000\n",
            "running reward: 16.29 at episode 22022, frame count 2100000\n",
            "running reward: 16.32 at episode 22041, frame count 2105000\n",
            "running reward: 14.90 at episode 22065, frame count 2110000\n",
            "running reward: 15.39 at episode 22086, frame count 2115000\n",
            "running reward: 17.90 at episode 22107, frame count 2120000\n",
            "running reward: 17.82 at episode 22128, frame count 2125000\n",
            "running reward: 16.49 at episode 22151, frame count 2130000\n",
            "running reward: 14.82 at episode 22171, frame count 2135000\n",
            "running reward: 13.08 at episode 22194, frame count 2140000\n",
            "running reward: 13.37 at episode 22211, frame count 2145000\n",
            "running reward: 14.04 at episode 22230, frame count 2150000\n",
            "running reward: 16.01 at episode 22254, frame count 2155000\n",
            "running reward: 15.54 at episode 22274, frame count 2160000\n",
            "running reward: 18.30 at episode 22291, frame count 2165000\n",
            "running reward: 18.08 at episode 22312, frame count 2170000\n",
            "running reward: 19.57 at episode 22335, frame count 2175000\n",
            "running reward: 18.76 at episode 22356, frame count 2180000\n",
            "running reward: 18.17 at episode 22375, frame count 2185000\n",
            "running reward: 15.94 at episode 22393, frame count 2190000\n",
            "running reward: 16.95 at episode 22410, frame count 2195000\n",
            "running reward: 16.08 at episode 22432, frame count 2200000\n",
            "running reward: 16.08 at episode 22450, frame count 2205000\n",
            "running reward: 18.35 at episode 22470, frame count 2210000\n",
            "running reward: 18.90 at episode 22491, frame count 2215000\n",
            "running reward: 17.51 at episode 22513, frame count 2220000\n",
            "running reward: 18.69 at episode 22532, frame count 2225000\n",
            "running reward: 19.43 at episode 22548, frame count 2230000\n",
            "running reward: 17.66 at episode 22569, frame count 2235000\n",
            "running reward: 19.00 at episode 22595, frame count 2240000\n",
            "running reward: 19.87 at episode 22613, frame count 2245000\n",
            "running reward: 21.02 at episode 22630, frame count 2250000\n",
            "running reward: 23.55 at episode 22651, frame count 2255000\n",
            "new best reward: 25.81 saving model\n",
            "running reward: 24.91 at episode 22670, frame count 2260000\n",
            "running reward: 22.73 at episode 22696, frame count 2265000\n",
            "running reward: 20.24 at episode 22735, frame count 2275000\n",
            "running reward: 14.52 at episode 22760, frame count 2280000\n",
            "running reward: 15.15 at episode 22781, frame count 2285000\n",
            "running reward: 16.80 at episode 22803, frame count 2290000\n",
            "running reward: 17.74 at episode 22820, frame count 2295000\n",
            "running reward: 14.91 at episode 22842, frame count 2300000\n",
            "running reward: 16.14 at episode 22859, frame count 2305000\n",
            "running reward: 17.00 at episode 22876, frame count 2310000\n",
            "running reward: 15.70 at episode 22897, frame count 2315000\n",
            "running reward: 14.89 at episode 22921, frame count 2320000\n",
            "running reward: 17.89 at episode 22940, frame count 2325000\n",
            "running reward: 17.00 at episode 22960, frame count 2330000\n",
            "running reward: 15.73 at episode 22982, frame count 2335000\n",
            "running reward: 16.34 at episode 23001, frame count 2340000\n",
            "running reward: 18.09 at episode 23020, frame count 2345000\n",
            "running reward: 13.34 at episode 23049, frame count 2350000\n",
            "running reward: 13.15 at episode 23074, frame count 2355000\n",
            "running reward: 12.12 at episode 23093, frame count 2360000\n",
            "running reward: 11.20 at episode 23116, frame count 2365000\n",
            "running reward: 12.71 at episode 23137, frame count 2370000\n",
            "running reward: 13.93 at episode 23157, frame count 2375000\n",
            "running reward: 14.40 at episode 23180, frame count 2380000\n",
            "running reward: 14.78 at episode 23205, frame count 2385000\n",
            "running reward: 14.37 at episode 23231, frame count 2390000\n",
            "running reward: 15.53 at episode 23249, frame count 2395000\n",
            "running reward: 15.17 at episode 23271, frame count 2400000\n",
            "running reward: 16.22 at episode 23292, frame count 2405000\n",
            "running reward: 15.04 at episode 23315, frame count 2410000\n",
            "running reward: 14.39 at episode 23332, frame count 2415000\n",
            "running reward: 13.40 at episode 23355, frame count 2420000\n",
            "running reward: 13.61 at episode 23375, frame count 2425000\n",
            "running reward: 16.63 at episode 23396, frame count 2430000\n",
            "running reward: 17.43 at episode 23420, frame count 2435000\n",
            "running reward: 17.14 at episode 23438, frame count 2440000\n",
            "running reward: 18.30 at episode 23455, frame count 2445000\n",
            "running reward: 18.64 at episode 23481, frame count 2450000\n",
            "running reward: 16.09 at episode 23503, frame count 2455000\n",
            "running reward: 14.48 at episode 23523, frame count 2460000\n",
            "running reward: 14.78 at episode 23540, frame count 2465000\n",
            "running reward: 14.67 at episode 23564, frame count 2470000\n",
            "running reward: 13.62 at episode 23584, frame count 2475000\n",
            "running reward: 14.25 at episode 23605, frame count 2480000\n",
            "running reward: 14.41 at episode 23630, frame count 2485000\n",
            "running reward: 14.93 at episode 23656, frame count 2490000\n",
            "running reward: 15.48 at episode 23682, frame count 2495000\n",
            "running reward: 17.42 at episode 23700, frame count 2500000\n",
            "running reward: 15.06 at episode 23726, frame count 2505000\n",
            "running reward: 15.27 at episode 23742, frame count 2510000\n",
            "running reward: 15.20 at episode 23766, frame count 2515000\n",
            "running reward: 14.23 at episode 23790, frame count 2520000\n",
            "running reward: 13.43 at episode 23805, frame count 2525000\n",
            "running reward: 14.59 at episode 23825, frame count 2530000\n",
            "running reward: 11.70 at episode 23850, frame count 2535000\n",
            "running reward: 11.70 at episode 23875, frame count 2540000\n",
            "running reward: 11.68 at episode 23897, frame count 2545000\n",
            "running reward: 10.96 at episode 23922, frame count 2550000\n",
            "running reward: 12.37 at episode 23951, frame count 2555000\n",
            "running reward: 14.46 at episode 23970, frame count 2560000\n",
            "running reward: 16.46 at episode 23991, frame count 2565000\n",
            "running reward: 16.31 at episode 24011, frame count 2570000\n",
            "running reward: 15.08 at episode 24032, frame count 2575000\n",
            "running reward: 17.80 at episode 24050, frame count 2580000\n",
            "running reward: 17.66 at episode 24071, frame count 2585000\n",
            "running reward: 17.52 at episode 24088, frame count 2590000\n",
            "running reward: 18.90 at episode 24103, frame count 2595000\n",
            "running reward: 18.65 at episode 24123, frame count 2600000\n",
            "running reward: 17.98 at episode 24143, frame count 2605000\n",
            "running reward: 17.66 at episode 24166, frame count 2610000\n",
            "running reward: 15.03 at episode 24191, frame count 2615000\n",
            "running reward: 15.18 at episode 24211, frame count 2620000\n",
            "running reward: 14.18 at episode 24235, frame count 2625000\n",
            "running reward: 14.64 at episode 24255, frame count 2630000\n",
            "running reward: 19.23 at episode 24282, frame count 2635000\n",
            "running reward: 21.32 at episode 24300, frame count 2640000\n",
            "running reward: 19.65 at episode 24323, frame count 2645000\n",
            "running reward: 20.44 at episode 24343, frame count 2650000\n",
            "running reward: 17.98 at episode 24366, frame count 2655000\n",
            "running reward: 14.53 at episode 24392, frame count 2660000\n",
            "running reward: 13.70 at episode 24417, frame count 2665000\n",
            "running reward: 15.11 at episode 24432, frame count 2670000\n",
            "running reward: 12.75 at episode 24456, frame count 2675000\n",
            "running reward: 13.32 at episode 24480, frame count 2680000\n",
            "running reward: 13.74 at episode 24501, frame count 2685000\n",
            "running reward: 15.87 at episode 24521, frame count 2690000\n",
            "running reward: 13.67 at episode 24546, frame count 2695000\n",
            "running reward: 15.26 at episode 24566, frame count 2700000\n",
            "running reward: 15.58 at episode 24586, frame count 2705000\n",
            "running reward: 18.87 at episode 24605, frame count 2710000\n",
            "running reward: 19.68 at episode 24622, frame count 2715000\n",
            "running reward: 22.47 at episode 24642, frame count 2720000\n",
            "running reward: 23.48 at episode 24660, frame count 2725000\n",
            "running reward: 23.01 at episode 24684, frame count 2730000\n",
            "running reward: 19.80 at episode 24708, frame count 2735000\n",
            "running reward: 14.42 at episode 24728, frame count 2740000\n",
            "running reward: 15.62 at episode 24747, frame count 2745000\n",
            "running reward: 14.84 at episode 24765, frame count 2750000\n",
            "running reward: 16.74 at episode 24786, frame count 2755000\n",
            "running reward: 15.48 at episode 24807, frame count 2760000\n",
            "running reward: 19.41 at episode 24827, frame count 2765000\n",
            "running reward: 18.27 at episode 24847, frame count 2770000\n",
            "running reward: 18.20 at episode 24866, frame count 2775000\n",
            "running reward: 17.92 at episode 24889, frame count 2780000\n",
            "running reward: 17.52 at episode 24912, frame count 2785000\n",
            "running reward: 16.88 at episode 24930, frame count 2790000\n",
            "running reward: 16.26 at episode 24950, frame count 2795000\n",
            "running reward: 16.20 at episode 24968, frame count 2800000\n",
            "running reward: 15.85 at episode 24988, frame count 2805000\n",
            "running reward: 16.69 at episode 25001, frame count 2810000\n",
            "running reward: 16.95 at episode 25025, frame count 2815000\n",
            "running reward: 16.67 at episode 25045, frame count 2820000\n",
            "running reward: 18.70 at episode 25066, frame count 2825000\n",
            "running reward: 18.27 at episode 25090, frame count 2830000\n",
            "running reward: 17.10 at episode 25109, frame count 2835000\n",
            "running reward: 21.54 at episode 25128, frame count 2840000\n",
            "running reward: 24.49 at episode 25150, frame count 2845000\n",
            "running reward: 22.61 at episode 25169, frame count 2850000\n",
            "running reward: 25.07 at episode 25186, frame count 2855000\n",
            "running reward: 24.78 at episode 25205, frame count 2860000\n",
            "new best reward: 26.58 saving model\n",
            "running reward: 22.08 at episode 25224, frame count 2865000\n",
            "running reward: 20.89 at episode 25248, frame count 2870000\n",
            "running reward: 18.33 at episode 25276, frame count 2875000\n",
            "running reward: 16.52 at episode 25299, frame count 2880000\n",
            "running reward: 13.71 at episode 25318, frame count 2885000\n",
            "running reward: 10.71 at episode 25346, frame count 2890000\n",
            "running reward: 9.75 at episode 25367, frame count 2895000\n",
            "running reward: 9.94 at episode 25388, frame count 2900000\n",
            "running reward: 11.98 at episode 25411, frame count 2905000\n",
            "running reward: 12.95 at episode 25433, frame count 2910000\n",
            "running reward: 12.37 at episode 25460, frame count 2915000\n",
            "running reward: 12.05 at episode 25484, frame count 2920000\n",
            "running reward: 12.62 at episode 25505, frame count 2925000\n",
            "running reward: 14.53 at episode 25530, frame count 2930000\n",
            "running reward: 15.18 at episode 25551, frame count 2935000\n",
            "running reward: 16.95 at episode 25569, frame count 2940000\n",
            "running reward: 21.74 at episode 25593, frame count 2945000\n",
            "running reward: 20.29 at episode 25612, frame count 2950000\n",
            "running reward: 17.59 at episode 25633, frame count 2955000\n",
            "running reward: 18.12 at episode 25651, frame count 2960000\n",
            "running reward: 16.31 at episode 25675, frame count 2965000\n",
            "running reward: 14.76 at episode 25697, frame count 2970000\n",
            "running reward: 16.54 at episode 25715, frame count 2975000\n",
            "running reward: 17.20 at episode 25738, frame count 2980000\n",
            "running reward: 17.45 at episode 25760, frame count 2985000\n",
            "running reward: 17.25 at episode 25786, frame count 2990000\n",
            "running reward: 15.28 at episode 25809, frame count 2995000\n",
            "running reward: 14.92 at episode 25830, frame count 3000000\n",
            "running reward: 14.36 at episode 25849, frame count 3005000\n",
            "running reward: 11.36 at episode 25877, frame count 3010000\n",
            "running reward: 13.28 at episode 25898, frame count 3015000\n",
            "running reward: 11.55 at episode 25919, frame count 3020000\n",
            "running reward: 11.19 at episode 25949, frame count 3025000\n",
            "running reward: 12.03 at episode 25969, frame count 3030000\n",
            "running reward: 11.42 at episode 25989, frame count 3035000\n",
            "running reward: 12.24 at episode 26009, frame count 3040000\n",
            "running reward: 12.92 at episode 26035, frame count 3045000\n",
            "running reward: 14.56 at episode 26055, frame count 3050000\n",
            "running reward: 14.78 at episode 26081, frame count 3055000\n",
            "running reward: 14.25 at episode 26104, frame count 3060000\n",
            "running reward: 14.87 at episode 26126, frame count 3065000\n",
            "running reward: 17.62 at episode 26145, frame count 3070000\n",
            "running reward: 14.88 at episode 26165, frame count 3075000\n",
            "running reward: 15.61 at episode 26186, frame count 3080000\n",
            "running reward: 15.49 at episode 26212, frame count 3085000\n",
            "running reward: 12.96 at episode 26235, frame count 3090000\n",
            "running reward: 10.98 at episode 26262, frame count 3095000\n",
            "running reward: 10.13 at episode 26283, frame count 3100000\n",
            "running reward: 10.57 at episode 26310, frame count 3105000\n",
            "running reward: 10.86 at episode 26332, frame count 3110000\n",
            "running reward: 18.18 at episode 26358, frame count 3115000\n",
            "running reward: 18.68 at episode 26378, frame count 3120000\n",
            "running reward: 19.02 at episode 26401, frame count 3125000\n",
            "running reward: 18.92 at episode 26424, frame count 3130000\n",
            "running reward: 15.25 at episode 26445, frame count 3135000\n",
            "running reward: 15.35 at episode 26466, frame count 3140000\n",
            "running reward: 14.32 at episode 26496, frame count 3145000\n",
            "running reward: 14.81 at episode 26515, frame count 3150000\n",
            "running reward: 14.13 at episode 26533, frame count 3155000\n",
            "running reward: 15.09 at episode 26557, frame count 3160000\n",
            "running reward: 17.49 at episode 26576, frame count 3165000\n",
            "running reward: 19.00 at episode 26599, frame count 3170000\n",
            "running reward: 17.32 at episode 26620, frame count 3175000\n",
            "running reward: 15.25 at episode 26640, frame count 3180000\n",
            "running reward: 13.60 at episode 26659, frame count 3185000\n",
            "running reward: 15.92 at episode 26674, frame count 3190000\n",
            "running reward: 13.49 at episode 26702, frame count 3195000\n",
            "running reward: 13.78 at episode 26723, frame count 3200000\n",
            "running reward: 12.51 at episode 26748, frame count 3205000\n",
            "running reward: 11.49 at episode 26768, frame count 3210000\n",
            "running reward: 12.37 at episode 26787, frame count 3215000\n",
            "running reward: 13.21 at episode 26811, frame count 3220000\n",
            "running reward: 13.99 at episode 26831, frame count 3225000\n",
            "running reward: 13.93 at episode 26856, frame count 3230000\n",
            "running reward: 14.36 at episode 26875, frame count 3235000\n",
            "running reward: 16.72 at episode 26896, frame count 3240000\n",
            "running reward: 16.78 at episode 26926, frame count 3245000\n",
            "running reward: 17.27 at episode 26945, frame count 3250000\n",
            "running reward: 17.83 at episode 26972, frame count 3255000\n",
            "running reward: 16.38 at episode 26989, frame count 3260000\n",
            "running reward: 16.76 at episode 27007, frame count 3265000\n",
            "running reward: 18.62 at episode 27035, frame count 3270000\n",
            "running reward: 14.65 at episode 27055, frame count 3275000\n",
            "running reward: 14.60 at episode 27079, frame count 3280000\n",
            "running reward: 12.94 at episode 27100, frame count 3285000\n",
            "running reward: 15.03 at episode 27120, frame count 3290000\n",
            "running reward: 17.98 at episode 27143, frame count 3295000\n",
            "running reward: 22.43 at episode 27163, frame count 3300000\n",
            "running reward: 25.12 at episode 27185, frame count 3305000\n",
            "new best reward: 27.0 saving model\n",
            "new best reward: 27.03 saving model\n",
            "new best reward: 27.14 saving model\n",
            "new best reward: 27.39 saving model\n",
            "new best reward: 27.47 saving model\n",
            "running reward: 27.47 at episode 27206, frame count 3310000\n",
            "new best reward: 27.56 saving model\n",
            "new best reward: 27.87 saving model\n",
            "running reward: 24.77 at episode 27232, frame count 3315000\n",
            "running reward: 21.78 at episode 27254, frame count 3320000\n",
            "running reward: 15.19 at episode 27276, frame count 3325000\n",
            "running reward: 13.16 at episode 27300, frame count 3330000\n",
            "running reward: 11.99 at episode 27325, frame count 3335000\n",
            "running reward: 14.10 at episode 27344, frame count 3340000\n",
            "running reward: 16.37 at episode 27369, frame count 3345000\n",
            "running reward: 14.94 at episode 27397, frame count 3350000\n",
            "running reward: 17.14 at episode 27417, frame count 3355000\n",
            "running reward: 17.86 at episode 27439, frame count 3360000\n",
            "running reward: 16.12 at episode 27465, frame count 3365000\n",
            "running reward: 15.99 at episode 27485, frame count 3370000\n",
            "running reward: 13.92 at episode 27506, frame count 3375000\n",
            "running reward: 14.77 at episode 27531, frame count 3380000\n",
            "running reward: 13.99 at episode 27557, frame count 3385000\n",
            "running reward: 14.63 at episode 27576, frame count 3390000\n",
            "running reward: 14.73 at episode 27596, frame count 3395000\n",
            "running reward: 12.34 at episode 27624, frame count 3400000\n",
            "running reward: 13.27 at episode 27648, frame count 3405000\n",
            "running reward: 11.41 at episode 27673, frame count 3410000\n",
            "running reward: 12.49 at episode 27701, frame count 3415000\n",
            "running reward: 14.59 at episode 27718, frame count 3420000\n",
            "running reward: 14.99 at episode 27738, frame count 3425000\n",
            "running reward: 18.64 at episode 27764, frame count 3430000\n",
            "running reward: 21.18 at episode 27784, frame count 3435000\n",
            "running reward: 21.40 at episode 27805, frame count 3440000\n",
            "running reward: 17.57 at episode 27830, frame count 3445000\n",
            "running reward: 18.54 at episode 27848, frame count 3450000\n",
            "running reward: 14.71 at episode 27875, frame count 3455000\n",
            "running reward: 16.90 at episode 27895, frame count 3460000\n",
            "running reward: 17.83 at episode 27920, frame count 3465000\n",
            "running reward: 17.48 at episode 27946, frame count 3470000\n",
            "running reward: 16.92 at episode 27965, frame count 3475000\n",
            "running reward: 17.04 at episode 27987, frame count 3480000\n",
            "running reward: 12.18 at episode 28011, frame count 3485000\n",
            "running reward: 11.06 at episode 28031, frame count 3490000\n",
            "running reward: 13.08 at episode 28058, frame count 3495000\n",
            "running reward: 14.05 at episode 28084, frame count 3500000\n",
            "running reward: 16.36 at episode 28104, frame count 3505000\n",
            "running reward: 16.69 at episode 28124, frame count 3510000\n",
            "running reward: 14.47 at episode 28145, frame count 3515000\n",
            "running reward: 14.00 at episode 28172, frame count 3520000\n",
            "running reward: 12.53 at episode 28191, frame count 3525000\n",
            "running reward: 12.00 at episode 28211, frame count 3530000\n",
            "running reward: 14.78 at episode 28235, frame count 3535000\n",
            "running reward: 15.21 at episode 28261, frame count 3540000\n",
            "running reward: 15.21 at episode 28280, frame count 3545000\n",
            "running reward: 15.74 at episode 28301, frame count 3550000\n",
            "running reward: 16.60 at episode 28325, frame count 3555000\n",
            "running reward: 16.67 at episode 28345, frame count 3560000\n",
            "running reward: 18.89 at episode 28370, frame count 3565000\n",
            "running reward: 19.13 at episode 28398, frame count 3570000\n",
            "running reward: 16.02 at episode 28422, frame count 3575000\n",
            "running reward: 13.01 at episode 28449, frame count 3580000\n",
            "running reward: 10.98 at episode 28469, frame count 3585000\n",
            "running reward: 10.96 at episode 28491, frame count 3590000\n",
            "running reward: 11.64 at episode 28511, frame count 3595000\n",
            "running reward: 14.09 at episode 28533, frame count 3600000\n",
            "running reward: 15.23 at episode 28558, frame count 3605000\n",
            "running reward: 14.11 at episode 28582, frame count 3610000\n",
            "running reward: 14.09 at episode 28603, frame count 3615000\n",
            "running reward: 16.23 at episode 28620, frame count 3620000\n",
            "running reward: 13.90 at episode 28641, frame count 3625000\n",
            "running reward: 14.94 at episode 28658, frame count 3630000\n",
            "running reward: 15.43 at episode 28682, frame count 3635000\n",
            "running reward: 17.00 at episode 28706, frame count 3640000\n",
            "running reward: 16.34 at episode 28729, frame count 3645000\n",
            "running reward: 16.84 at episode 28750, frame count 3650000\n",
            "running reward: 16.80 at episode 28771, frame count 3655000\n",
            "running reward: 16.65 at episode 28793, frame count 3660000\n",
            "running reward: 16.12 at episode 28818, frame count 3665000\n",
            "running reward: 16.06 at episode 28839, frame count 3670000\n",
            "running reward: 18.93 at episode 28856, frame count 3675000\n",
            "running reward: 16.90 at episode 28880, frame count 3680000\n",
            "running reward: 16.02 at episode 28906, frame count 3685000\n",
            "running reward: 18.39 at episode 28931, frame count 3690000\n",
            "running reward: 13.45 at episode 28951, frame count 3695000\n",
            "running reward: 13.84 at episode 28975, frame count 3700000\n",
            "running reward: 15.86 at episode 28996, frame count 3705000\n",
            "running reward: 12.89 at episode 29025, frame count 3710000\n",
            "running reward: 13.08 at episode 29049, frame count 3715000\n",
            "running reward: 13.82 at episode 29072, frame count 3720000\n",
            "running reward: 12.48 at episode 29095, frame count 3725000\n",
            "running reward: 17.53 at episode 29116, frame count 3730000\n",
            "running reward: 16.52 at episode 29141, frame count 3735000\n",
            "running reward: 17.42 at episode 29167, frame count 3740000\n",
            "running reward: 18.14 at episode 29186, frame count 3745000\n",
            "running reward: 16.15 at episode 29207, frame count 3750000\n",
            "running reward: 14.86 at episode 29231, frame count 3755000\n",
            "running reward: 14.11 at episode 29254, frame count 3760000\n",
            "running reward: 13.36 at episode 29272, frame count 3765000\n",
            "running reward: 11.57 at episode 29298, frame count 3770000\n",
            "running reward: 13.30 at episode 29320, frame count 3775000\n",
            "running reward: 12.49 at episode 29344, frame count 3780000\n",
            "running reward: 12.93 at episode 29365, frame count 3785000\n",
            "running reward: 13.17 at episode 29384, frame count 3790000\n",
            "running reward: 12.59 at episode 29406, frame count 3795000\n",
            "running reward: 14.48 at episode 29428, frame count 3800000\n",
            "running reward: 16.17 at episode 29455, frame count 3805000\n",
            "running reward: 16.92 at episode 29478, frame count 3810000\n"
          ]
        }
      ],
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "# improves training time\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "\n",
        "\n",
        "#open and read the file after the appending:\n",
        "f = open(best_score_filepath, \"r\")\n",
        "best_score = float(f.read())\n",
        "f.close()\n",
        "\n",
        "\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 30000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 5000\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "while True:  # Run episodes until episode reward goes above threshold\n",
        "    # get initial state from environment. State is a (84, 84, 4) 'lazyframes' object. wrap with np.array().\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    # 'timestep' is a frame in an episode.\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "        \n",
        "        # Use epsilon-greedy for exploration\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # choose best action from policy network\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            # action_probs is 4 long array with weights to take each action\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action from output of policy network \n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            # this is the Q value from replay memory.\n",
        "            # adding the reward here is what tells the network if it was a good or bad move. higher reward, do this more.\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values (don't change weights for actions that weren't taken)\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            # Train the model on the states and updated Q-values\n",
        "            with tf.GradientTape() as tape:\n",
        "                # predicted best move from policy network\n",
        "                q_values = model(state_sample)\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                \n",
        "                # Calculate loss between new Q-value (obtained from replay memory) and old Q-value (predicted from network)\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "                # Backpropagation\n",
        "                grads = tape.gradient(loss, model.trainable_variables)\n",
        "            \n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > best_score:\n",
        "      best_score = running_reward\n",
        "      f = open(best_score_filepath, \"w\")\n",
        "      f.write(str(best_score))\n",
        "      f.close()\n",
        "      print(f\"new best reward: {best_score} saving model\")\n",
        "      model.save_weights(model_filepath)\n",
        "\n",
        "\n",
        "    if running_reward > 40:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        model.save_weights(model_filepath)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2JG1cokV4x7"
      },
      "source": [
        "## Play:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laTEQWtQ51AF"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "IUmv2FSBLRHv",
        "outputId": "47d72e42-79ce-4277-bf70-55e32bf1ecc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ad47e32428cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_atari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BreakoutNoFrameskip-v4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_deepmind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_stack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_q_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'make_atari' is not defined"
          ]
        }
      ],
      "source": [
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "env.seed(42)\n",
        "\n",
        "model = create_q_model()\n",
        "model.load_weights(model_filepath)\n",
        "\n",
        "state = np.array(env.reset())\n",
        "total_reward = 0\n",
        "frame_count = 4\n",
        "done = False\n",
        "# graph_frames = True\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ims = []\n",
        "\n",
        "while not done:\n",
        "  state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
        "  action_probs = model(state_tensor, training=False)\n",
        "  action = tf.argmax(action_probs[0]).numpy()  \n",
        "  state, reward, done, _ = env.step(action)\n",
        "  total_reward += reward  \n",
        "\n",
        "  im = ax.imshow(state, animated=True)\n",
        "  if frame_count == 0:\n",
        "      ax.imshow(state)  # show an initial one first\n",
        "  ims.append([im])\n",
        "  frame_count += 1\n",
        "  # if graph_frames:\n",
        "  #   plt.axis(\"off\")\n",
        "  #   plt.imshow(state)\n",
        "  #   plt.figure()\n",
        "print(f\"frames played: {frame_count} total reward: {total_reward}\")\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
        "                                repeat_delay=1000)\n",
        "ani.save(movie_save_path)\n",
        "\n",
        "mp4 = open(movie_save_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JrkN4VS3VxFT",
        "t2JG1cokV4x7"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}