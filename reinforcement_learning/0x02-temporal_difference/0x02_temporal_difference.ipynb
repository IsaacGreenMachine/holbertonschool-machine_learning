{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Gozq0rJgEFxl",
        "1iT2XI-5cEi4",
        "IXrrZGobAaz3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[toy_text]\n",
        "import gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaoUnTxOELur",
        "outputId": "0d4eb505-8b8d-47a7-daa3-65d056cafd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (5.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (4.1.1)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Monte Carlo"
      ],
      "metadata": {
        "id": "Gozq0rJgEFxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xe2AfIQDxPl"
      },
      "outputs": [],
      "source": [
        "# V[x] += alpha * (G[x] - V[x])\n",
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"\n",
        "    performs the Monte Carlo algorithm\n",
        "    env is the openAI environment instance\n",
        "    V is a numpy.ndarray of shape (s,) containing the value estimate\n",
        "    policy is a function that takes in a state and returns the next action to take\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    Returns: V, the updated value estimate\n",
        "    \"\"\"\n",
        "    for episode in range(episodes):\n",
        "      results_list = []\n",
        "      state = env.reset()\n",
        "      #print(env.render()[0])\n",
        "      for frame in range(max_steps):\n",
        "        action = policy(state)\n",
        "        nxt_state, reward, terminate, _ = env.step(action)\n",
        "        results_list.append((state, reward))\n",
        "        if terminate or frame > max_steps:\n",
        "          break\n",
        "        state = nxt_state\n",
        "      \n",
        "      results_list = np.array(results_list, dtype=int)\n",
        "      G = 0\n",
        "      # replay episode backwards, applying discount rate each time\n",
        "      for (s, r) in results_list[::-1]:\n",
        "        G = (gamma * G) + r\n",
        "        # first visit approach\n",
        "        if s not in results_list[:episode, 0]:\n",
        "          V[s] += alpha * (G - V[s])\n",
        "    return V.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1', render_mode=\"ansi\")\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
        "np.set_printoptions(precision=2)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GPP_vclxDiJ",
        "outputId": "fc441532-a2dc-4263-b8d8-7d0194439b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.9   0.73  0.66  0.73  0.9   0.9   0.59  0.53]\n",
            " [ 0.59  0.66  0.73  0.81  0.66  0.39  0.48  0.39]\n",
            " [ 0.66  0.25  0.35 -1.    1.    0.48  0.43  0.43]\n",
            " [ 0.9   0.43  0.28  0.59  0.9  -1.    0.48  0.48]\n",
            " [ 1.    0.73  0.59 -1.    1.    1.    0.73  0.73]\n",
            " [ 1.   -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
            " [ 1.   -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
            " [ 1.    1.    1.   -1.    1.    1.    1.    1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
        " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
        " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
        " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
        " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
        " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
        " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
        " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n",
        "'''\n",
        "\n",
        "'''\n",
        "[[ 0.9   0.73  0.66  0.73  0.9   0.9   0.59  0.53]\n",
        " [ 0.59  0.66  0.73  0.81  0.66  0.39  0.48  0.39]\n",
        " [ 0.66  0.25  0.35 -1.    1.    0.48  0.43  0.43]\n",
        " [ 0.9   0.43  0.28  0.59  0.9  -1.    0.48  0.48]\n",
        " [ 1.    0.73  0.59 -1.    1.    1.    0.73  0.73]\n",
        " [ 1.   -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
        " [ 1.   -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
        " [ 1.    1.    1.   -1.    1.    1.    1.    1.  ]]\n",
        "'''"
      ],
      "metadata": {
        "id": "PGHzuNRMERSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bUZEEFZzD2-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD(λ)"
      ],
      "metadata": {
        "id": "1iT2XI-5cEi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "  \"\"\"\n",
        "  performs the TD(λ) algorithm\n",
        "  env is the openAI environment instance\n",
        "  V is a numpy.ndarray of shape (s,) containing the value estimate\n",
        "  policy is a function that takes in a state and returns the next action to take\n",
        "  lambtha is the eligibility trace factor\n",
        "  episodes is the total number of episodes to train over\n",
        "  max_steps is the maximum number of steps per episode\n",
        "  alpha is the learning rate\n",
        "  gamma is the discount rate\n",
        "  Returns: V, the updated value estimate\n",
        "  \"\"\"\n",
        "  elig_trace = np.zeros(V.shape[0])\n",
        "  for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    #print(env.render()[0])\n",
        "    for frame in range(max_steps):\n",
        "      action = policy(state)\n",
        "      nxt_state, reward, terminate, _ = env.step(action)\n",
        "      elig_trace *= (gamma * lambtha)\n",
        "      elig_trace[state] += 1\n",
        "\n",
        "      V += alpha * (reward + (gamma * V[nxt_state]) - V[state]) * elig_trace\n",
        "            \n",
        "      if terminate or frame > max_steps:\n",
        "        break\n",
        "      \n",
        "      state = nxt_state\n",
        "  return V"
      ],
      "metadata": {
        "id": "TQTIPoKAcLUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
        "np.set_printoptions(precision=4)\n",
        "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzI3Lnm3cN_a",
        "outputId": "56addc22-b0e8-4ae8-b39b-5c07ec7064ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.8646 -0.8508 -0.8404 -0.7797 -0.6912 -0.6594 -0.5903 -0.5297]\n",
            " [-0.8924 -0.8899 -0.878  -0.8919 -0.8141 -0.7076 -0.6863 -0.5484]\n",
            " [-0.9237 -0.9204 -0.9442 -1.     -0.9273 -0.7414 -0.6061 -0.3766]\n",
            " [-0.9291 -0.9221 -0.9604 -0.979  -0.9795 -1.     -0.6747 -0.4439]\n",
            " [-0.9444 -0.9726 -0.9697 -1.     -0.9302 -0.8485 -0.642  -0.3288]\n",
            " [-0.8948 -1.     -1.      0.5021 -0.9561 -0.9146 -1.      0.2218]\n",
            " [-0.9057 -1.     -0.4269  0.5536 -1.     -0.78   -1.      0.7115]\n",
            " [-0.903  -0.9503 -0.8844 -1.      1.     -0.3773  0.1801  1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[[ 0.5314  0.5905  0.3138  0.3138  0.6561  0.9     0.81    0.9   ]\n",
        " [ 0.5314  0.5905  0.4783  0.6561  0.5905  0.6561  0.6561  0.5314]\n",
        " [ 0.6561  0.729   0.5905 -1.      0.9     0.9     0.5905  0.3874]\n",
        " [ 0.729   0.81    0.81    0.9     1.     -1.      0.5314  0.4305]\n",
        " [ 0.5905  0.6561  0.81   -1.      1.      1.      0.729   0.4783]\n",
        " [ 0.9    -1.     -1.      1.      1.      1.     -1.      0.81  ]\n",
        " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
        " [ 0.9     0.81    1.     -1.      1.      1.      1.      1.    ]]\n",
        "'''"
      ],
      "metadata": {
        "id": "7ZOnKFxtcR62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA(λ)"
      ],
      "metadata": {
        "id": "IXrrZGobAaz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "    \"\"\"\n",
        "    performs SARSA(λ):\n",
        "\n",
        "    env is the openAI environment instance\n",
        "    Q is a numpy.ndarray of shape (s,a) containing the Q table\n",
        "    lambtha is the eligibility trace factor\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    epsilon is the initial threshold for epsilon greedy\n",
        "    min_epsilon is the minimum value that epsilon should decay to\n",
        "    epsilon_decay is the decay rate for updating epsilon between episodes\n",
        "    Returns: Q, the updated Q table\n",
        "    \"\"\"\n",
        "    elig_traces = np.zeros_like(Q)\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        if np.random.uniform() < epsilon:\n",
        "            action = np.random.randint(0, Q.shape[1])\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])\n",
        "        for step in range(max_steps):\n",
        "            next_state, reward, terminate, _ = env.step(action)\n",
        "            if np.random.uniform() < epsilon:\n",
        "              action2 = np.random.randint(0, Q.shape[1])\n",
        "            else:\n",
        "              action2 = np.argmax(Q[next_state, :])\n",
        "          \n",
        "            delta = reward + gamma * Q[next_state, action2] - Q[state, action]\n",
        "            elig_traces[state, action] += 1\n",
        "\n",
        "            Q += alpha * delta * elig_traces\n",
        "            elig_traces *= lambtha * gamma\n",
        "\n",
        "            state = next_state\n",
        "            action = action2\n",
        "            if terminate:\n",
        "                break\n",
        "        epsilon = min_epsilon + (1 - min_epsilon) * np.exp(-1 * epsilon_decay * episode)\n",
        "    \n",
        "    return Q"
      ],
      "metadata": {
        "id": "22QiEVDRAeYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "env = gym.make('FrozenLake8x8-v1')\n",
        "Q = np.random.uniform(size=(64, 4))\n",
        "np.set_printoptions(precision=4)\n",
        "print(sarsa_lambtha(env, Q, 0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6qDiuudAiWB",
        "outputId": "b334d2ff-6fa6-489b-c1dd-98825a847458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5945 0.5181 0.6    0.5935]\n",
            " [0.5541 0.6034 0.5546 0.5923]\n",
            " [0.5843 0.6234 0.5557 0.6015]\n",
            " [0.5913 0.6093 0.6021 0.6462]\n",
            " [0.5698 0.6021 0.6952 0.5798]\n",
            " [0.6378 0.6463 0.7073 0.6227]\n",
            " [0.6435 0.7124 0.7074 0.6367]\n",
            " [0.7213 0.6751 0.6702 0.6721]\n",
            " [0.6123 0.6136 0.6114 0.6001]\n",
            " [0.6256 0.6044 0.6144 0.5959]\n",
            " [0.6061 0.6079 0.6586 0.6014]\n",
            " [0.4624 0.4572 0.4363 0.6766]\n",
            " [0.5598 0.5404 0.7211 0.5365]\n",
            " [0.6648 0.6871 0.7408 0.6352]\n",
            " [0.7558 0.7017 0.7004 0.6817]\n",
            " [0.5548 0.5287 0.7293 0.5583]\n",
            " [0.6184 0.6236 0.6241 0.6202]\n",
            " [0.6611 0.6974 0.6432 0.6507]\n",
            " [0.6903 0.5452 0.547  0.4837]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.4604 0.4928 0.7766 0.4787]\n",
            " [0.6738 0.7131 0.8178 0.673 ]\n",
            " [0.766  0.8042 0.7492 0.7227]\n",
            " [0.6743 0.7893 0.6717 0.6535]\n",
            " [0.6735 0.6367 0.6685 0.6576]\n",
            " [0.6984 0.7666 0.6821 0.6527]\n",
            " [0.7538 0.6166 0.6692 0.6471]\n",
            " [0.6015 0.6684 0.6531 0.485 ]\n",
            " [0.6615 0.5575 0.813  0.5578]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.8117 0.7689 0.7826 0.8222]\n",
            " [0.7439 0.8192 0.6834 0.7238]\n",
            " [0.6416 0.6698 0.6452 0.704 ]\n",
            " [0.8663 0.7444 0.6972 0.6875]\n",
            " [0.6779 0.7156 0.6752 0.8383]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.7553 0.8672 0.4854 0.7223]\n",
            " [0.7457 0.779  0.4025 0.8308]\n",
            " [0.5771 0.8082 0.614  0.8185]\n",
            " [0.8717 0.7347 0.7154 0.4264]\n",
            " [0.6471 0.608  0.7112 0.6592]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.2    0.0666 0.7178 0.2239]\n",
            " [0.5456 0.7444 0.8326 0.1181]\n",
            " [0.3994 0.7686 0.7108 0.3511]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [1.0323 0.4881 0.6528 0.4672]\n",
            " [0.4679 0.4363 0.4668 0.6044]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.3118 0.6329 0.3778 0.1796]\n",
            " [0.0247 0.0672 0.6461 0.4537]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.6731 0.3215 0.0207 0.821 ]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.629  1.2083 0.4323 0.7469]\n",
            " [0.3086 0.4458 0.4806 0.3747]\n",
            " [0.4756 0.4818 0.3714 0.3462]\n",
            " [0.561  0.0679 0.2739 0.4344]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.8639 0.1175 0.5174 0.1321]\n",
            " [0.75   0.3961 0.5654 0.1833]\n",
            " [0.1448 0.4881 0.3556 0.9404]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[[0.5452 0.5363 0.6315 0.5329]\n",
        " [0.5591 0.6166 0.5316 0.5425]\n",
        " [0.5336 0.602  0.529  0.5463]\n",
        " [0.5475 0.5974 0.5362 0.5436]\n",
        " [0.5531 0.5693 0.6117 0.568 ]\n",
        " [0.6147 0.6011 0.6511 0.5966]\n",
        " [0.6472 0.6183 0.599  0.6176]\n",
        " [0.6334 0.6267 0.6519 0.634 ]\n",
        " [0.5571 0.5233 0.646  0.5867]\n",
        " [0.6456 0.5602 0.545  0.5321]\n",
        " [0.6303 0.53   0.5055 0.5394]\n",
        " [0.4495 0.4853 0.4384 0.5781]\n",
        " [0.5291 0.5351 0.5489 0.5821]\n",
        " [0.6182 0.6166 0.6186 0.6047]\n",
        " [0.6266 0.5832 0.6497 0.5645]\n",
        " [0.5369 0.3657 0.7081 0.4936]\n",
        " [0.5924 0.7393 0.5806 0.5818]\n",
        " [0.5621 0.7052 0.5681 0.5429]\n",
        " [0.6894 0.509  0.4663 0.5361]\n",
        " [0.2828 0.1202 0.2961 0.1187]\n",
        " [0.4457 0.4633 0.411  0.5208]\n",
        " [0.5899 0.6983 0.7595 0.5963]\n",
        " [0.7263 0.699  0.6698 0.6954]\n",
        " [0.6126 0.7508 0.4898 0.4768]\n",
        " [0.6615 0.5872 0.7568 0.5987]\n",
        " [0.5805 0.5433 0.5839 0.7284]\n",
        " [0.6236 0.6239 0.7243 0.5689]\n",
        " [0.6498 0.7383 0.6077 0.5422]\n",
        " [0.6334 0.6377 0.7003 0.6311]\n",
        " [0.8811 0.5813 0.8817 0.6925]\n",
        " [0.7557 0.7478 0.7796 0.7706]\n",
        " [0.6687 0.8253 0.65   0.5062]\n",
        " [0.6277 0.7568 0.6078 0.6561]\n",
        " [0.6366 0.6973 0.6338 0.7487]\n",
        " [0.7121 0.7965 0.7082 0.7455]\n",
        " [0.8965 0.3676 0.4359 0.8919]\n",
        " [0.7498 0.8535 0.3625 0.7401]\n",
        " [0.7681 0.7448 0.2974 0.837 ]\n",
        " [0.4996 0.6835 0.4382 0.8703]\n",
        " [0.8936 0.7053 0.4904 0.3181]\n",
        " [0.6677 0.7224 0.8078 0.6766]\n",
        " [0.9755 0.8558 0.0117 0.36  ]\n",
        " [0.73   0.1716 0.521  0.0543]\n",
        " [0.2466 0.0813 0.8518 0.2852]\n",
        " [0.3454 0.8602 0.7229 0.1075]\n",
        " [0.2801 0.7741 0.6684 0.288 ]\n",
        " [0.9342 0.614  0.5356 0.5899]\n",
        " [1.0137 0.391  0.4284 0.2431]\n",
        " [0.382  0.4696 0.4571 0.599 ]\n",
        " [0.2274 0.2544 0.058  0.4344]\n",
        " [0.3118 0.6755 0.4197 0.1796]\n",
        " [0.0247 0.0672 0.778  0.4537]\n",
        " [0.5366 0.8967 0.9903 0.2169]\n",
        " [0.6914 0.3132 0.0996 0.7817]\n",
        " [0.32   0.3835 0.5883 0.831 ]\n",
        " [0.629  1.3232 0.2735 0.8131]\n",
        " [0.2803 0.5022 0.5382 0.2851]\n",
        " [0.6295 0.6324 0.2997 0.2133]\n",
        " [0.5699 0.0643 0.2075 0.4247]\n",
        " [0.3742 0.4636 0.2776 0.5868]\n",
        " [0.8639 0.1175 0.5174 0.1321]\n",
        " [0.7169 0.3961 0.5654 0.1833]\n",
        " [0.1448 0.4881 0.3556 0.9404]\n",
        " [0.7653 0.7487 0.9037 0.0834]]\n",
        " '''"
      ],
      "metadata": {
        "id": "ada256EpAklK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}