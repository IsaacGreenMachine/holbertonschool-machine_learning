{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "otVPXwjQ4Ca4",
        "N0UTKkiC4F63",
        "WBGnTrqn4HQh",
        "7GaqkTmx4Uy9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoZwetny2NNN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import transformers\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zendesk_articles = \"/content/drive/MyDrive/Colab Notebooks/Machine Learning/data/ZendeskArticles\""
      ],
      "metadata": {
        "id": "SLxMa-xa5_GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0-qa.py"
      ],
      "metadata": {
        "id": "mgviNP2h3-gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answer(question, reference):\n",
        "    \"\"\"\n",
        "    finds a snippet of text within a reference document to answer a question:\n",
        "\n",
        "    question - a string containing the question to answer\n",
        "    reference - a string containing the reference document from which to find the answer\n",
        "    Returns: a string containing the answer (or None if no answer found)\n",
        "    \n",
        "    Uses bert-uncased-tf2-qa model from the tensorflow-hub library\n",
        "    uses the pre-trained BertTokenizer, bert-large-uncased-whole-word-masking-finetuned-squad, from the transformers library.\n",
        "    \"\"\"\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    paragraph_tokens = tokenizer.tokenize(reference)\n",
        "    tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
        "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_word_ids)\n",
        "    input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
        "\n",
        "    input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
        "        tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
        "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "    # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
        "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "    answer_tokens = tokens[short_start: short_end + 1]\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "    # print(f'Question: {question}')\n",
        "    # print(f'Answer: {answer}')\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "2siAH7kx3_eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(zendesk_articles + '/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "print(question_answer('When are PLDs?', reference))\n"
      ],
      "metadata": {
        "id": "mJ5bluVx3__J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d793a1e-31c4-461c-b057-324f7790657e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on - site days from 9 : 00 am to 3 : 00 pm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Create the loop"
      ],
      "metadata": {
        "id": "otVPXwjQ4Ca4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  print(\"Q: \", end=\"\")\n",
        "  inpt = input()\n",
        "  if inpt.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "    print(\"A: Goodbye\")\n",
        "    break\n",
        "  print(f\"A: \")"
      ],
      "metadata": {
        "id": "ihpfY3JO4DL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf70680-2e44-4d4b-9b2a-d84e910a7dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Hello\n",
            "A: \n",
            "Q: How are you?\n",
            "A: \n",
            "Q: BYE\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Answer Questions"
      ],
      "metadata": {
        "id": "N0UTKkiC4F63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_loop(reference):\n",
        "    \"\"\"\n",
        "    reference is the reference text\n",
        "    If the answer cannot be found in the reference text, respond with Sorry, I do not understand your question.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "    paragraph_tokens = tokenizer.tokenize(reference)\n",
        "\n",
        "    while True:\n",
        "      print(\"Q: \", end=\"\")\n",
        "      inpt = input()      \n",
        "      if inpt.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "        print(\"A: Goodbye\")\n",
        "        break\n",
        "\n",
        "      question_tokens = tokenizer.tokenize(inpt)\n",
        "      \n",
        "      tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
        "      input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      input_mask = [1] * len(input_word_ids)\n",
        "      input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
        "      input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
        "          tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
        "      outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "      # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
        "      short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "      short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "      answer_tokens = tokens[short_start: short_end + 1]\n",
        "      answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "      # print(f'Question: {question}')\n",
        "      # print(f'Answer: {answer}')\n",
        "      if answer:\n",
        "        print(f\"A: {answer}\")\n",
        "      else:\n",
        "        print(\"A: Sorry, I do not understand your question.\")"
      ],
      "metadata": {
        "id": "bqTY2eVG4G1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(zendesk_articles+'/PeerLearningDays.md') as f:\n",
        "    reference = f.read()\n",
        "\n",
        "answer_loop(reference)"
      ],
      "metadata": {
        "id": "yPGkQgig4G_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e38c3c-a392-49a1-873b-8c46d3c9b56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What time are PLDs?\n",
            "A: 9 : 00 am to 3 : 00 pm\n",
            "Q: What are Mock Interviews?\n",
            "A: Sorry, I do not understand your question.\n",
            "Q: What does PLD stand for?\n",
            "A: peer learning days\n",
            "Q: EXIT\n",
            "A: Goodbye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Semantic Search"
      ],
      "metadata": {
        "id": "WBGnTrqn4HQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(corpus_path, sentence):\n",
        "    \"\"\"\n",
        "    that performs semantic search on a corpus of documents:\n",
        "\n",
        "    corpus_path is the path to the corpus of reference documents on which to perform semantic search\n",
        "    sentence is the sentence from which to perform semantic search\n",
        "    Returns: the reference text of the document most similar to sentence\n",
        "    \"\"\"\n",
        "    docs = [sentence]\n",
        "    for doc in os.listdir(corpus_path):\n",
        "      if doc[-3:] == \".md\":\n",
        "        with open(zendesk_articles + \"/\" + doc) as f:\n",
        "          docs.append(f.read())\n",
        "    univ_sent_enc = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
        "    embeddings = univ_sent_enc(docs)\n",
        "    cosine_sims = []\n",
        "    for out in embeddings:\n",
        "        cosine_sims.append(np.dot(embeddings[0], out)/(np.linalg.norm(embeddings[0])*np.linalg.norm(out)))\n",
        "    docnum = np.array(cosine_sims[1:]).argmax() + 1\n",
        "    return docs[docnum]"
      ],
      "metadata": {
        "id": "xs0Elwbk4KKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(semantic_search(zendesk_articles, 'When are PLDs?'))"
      ],
      "metadata": {
        "id": "Ksc3DG6v4KZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a117ff-73af-4978-c9a0-af7a33ace514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLD Overview\n",
            "Peer Learning Days (PLDs) are a time for you and your peers to ensure that each of you understands the concepts you've encountered in your projects, as well as a time for everyone to collectively grow in technical, professional, and soft skills. During PLD, you will collaboratively review prior projects with a group of cohort peers.\n",
            "PLD Basics\n",
            "PLDs are mandatory on-site days from 9:00 AM to 3:00 PM. If you cannot be present or on time, you must use a PTO. \n",
            "No laptops, tablets, or screens are allowed until all tasks have been whiteboarded and understood by the entirety of your group. This time is for whiteboarding, dialogue, and active peer collaboration. After this, you may return to computers with each other to pair or group program. \n",
            "Peer Learning Days are not about sharing solutions. This doesn't empower peers with the ability to solve problems themselves! Peer learning is when you share your thought process, whether through conversation, whiteboarding, debugging, or live coding. \n",
            "When a peer has a question, rather than offering the solution, ask the following:\n",
            "\"How did you come to that conclusion?\"\n",
            "\"What have you tried?\"\n",
            "\"Did the man page give you a lead?\"\n",
            "\"Did you think about this concept?\"\n",
            "Modeling this form of thinking for one another is invaluable and will strengthen your entire cohort.\n",
            "Your ability to articulate your knowledge is a crucial skill and will be required to succeed during technical interviews and through your career. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-reference Question Answering"
      ],
      "metadata": {
        "id": "7GaqkTmx4Uy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answer(corpus_path):\n",
        "    \"\"\"\n",
        "    answers questions from multiple reference texts:\n",
        "    corpus_path is the path to the corpus of reference documents\n",
        "    \"\"\"\n",
        "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
        "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    docs = []\n",
        "    for doc in os.listdir(corpus_path):\n",
        "      if doc[-3:] == \".md\":\n",
        "        with open(zendesk_articles + \"/\" + doc) as f:\n",
        "          docs.append(f.read())\n",
        "    univ_sent_enc = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
        "    embeddings = univ_sent_enc(docs)\n",
        "\n",
        "    while True:\n",
        "      print(\"Q: \", end=\"\")\n",
        "      inpt = input()      \n",
        "      if inpt.lower() in [\"exit\", \"quit\", \"goodbye\", \"bye\"]:\n",
        "        print(\"A: Goodbye\")\n",
        "        break\n",
        "      \n",
        "\n",
        "      new_embedding = univ_sent_enc([inpt])\n",
        "      cosine_sims = []\n",
        "      for out in embeddings:\n",
        "          cosine_sims.append(np.dot(new_embedding, out)/(np.linalg.norm(new_embedding)*np.linalg.norm(out)))\n",
        "      docnum = np.array(cosine_sims).argmax()\n",
        "      paragraph_tokens = tokenizer.tokenize(docs[docnum])\n",
        "      question_tokens = tokenizer.tokenize(inpt)\n",
        "      tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n",
        "      input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      input_mask = [1] * len(input_word_ids)\n",
        "      input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n",
        "      input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n",
        "          tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n",
        "      outputs = model([input_word_ids, input_mask, input_type_ids])\n",
        "      # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n",
        "      short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
        "      short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
        "      answer_tokens = tokens[short_start: short_end + 1]\n",
        "      answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "      # print(f'Question: {question}')\n",
        "      # print(f'Answer: {answer}')\n",
        "      if answer:\n",
        "        print(f\"A: {answer}\")\n",
        "      else:\n",
        "        print(\"A: Sorry, I do not understand your question.\")"
      ],
      "metadata": {
        "id": "Zu81Hti04X7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer(zendesk_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOhELRX95F2P",
        "outputId": "c71f8972-c583-400e-f159-6b532e7e39a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: When are PLDs?\n",
            "A: on - site days from 9 : 00 am to 3 : 00 pm\n",
            "Q: What are Mock Interviews?\n",
            "A: help you train for technical interviews\n",
            "Q: What does PLD stand for?\n",
            "A: peer learning days\n",
            "Q: goodbye\n",
            "A: Goodbye\n"
          ]
        }
      ]
    }
  ]
}