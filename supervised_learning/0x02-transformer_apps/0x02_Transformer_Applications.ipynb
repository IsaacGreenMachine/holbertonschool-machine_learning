{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ErKgGKL0GFHk",
        "99pjplzHGIWl",
        "Iawlxg6QGMY7",
        "juyrMsOyGOqw",
        "7bEhci9fGT5w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhOMHeB3F_Mm"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf \n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Dataset"
      ],
      "metadata": {
        "id": "ErKgGKL0GFHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt2en_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "for pt, en in pt2en_train.take(1):\n",
        "  print(pt.numpy().decode('utf-8'))\n",
        "  print(en.numpy().decode('utf-8'))\n",
        "\n",
        "\n",
        "# e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
        "# and when you improve searchability , you actually take away the one advantage of print , which is serendipity ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yvq5pZnGHqk",
        "outputId": "e323459c-e755-4b68-a782-872965ae8590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pt2en_train.as_numpy_iterator())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI8vj5g00ax8",
        "outputId": "33fe6fcb-95b6-4bde-fad3-fff159aea6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.dataset_ops._NumpyIterator object at 0x7f3ea8edce50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        creates the instance attributes:\n",
        "            data_train - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset train split, loaded as_supervided\n",
        "            data_valid - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split, loaded as_supervided\n",
        "            tokenizer_pt - Portuguese tokenizer created from the training set\n",
        "            tokenizer_en - English tokenizer created from the training set\n",
        "        \"\"\"\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        creates sub-word tokenizers for dataset\n",
        "        \n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The maximum vocab size should be set to 2**15\n",
        "        \n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n",
        "        \"\"\"\n",
        "        f = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
        "        en_tok = f((en.numpy() for _, en in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        pt_tok = f((pt.numpy() for pt, _ in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        return pt_tok, en_tok"
      ],
      "metadata": {
        "id": "3Mgl5qmvGIFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt.numpy().decode('utf-8'))\n",
        "    print(en.numpy().decode('utf-8'))\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt.numpy().decode('utf-8'))\n",
        "    print(en.numpy().decode('utf-8'))\n",
        "print(type(data.tokenizer_pt))\n",
        "print(type(data.tokenizer_en))\n",
        "\n",
        "\n",
        "# e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
        "# and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
        "# tinham comido peixe com batatas fritas ?\n",
        "# did they eat fish and chips ?\n",
        "# <class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n",
        "# <class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1rBFKHJfIVn",
        "outputId": "e3627205-0069-4d8a-f0e8-a39d17070bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "tinham comido peixe com batatas fritas ?\n",
            "did they eat fish and chips ?\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Encode Tokens"
      ],
      "metadata": {
        "id": "99pjplzHGIWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        creates the instance attributes:\n",
        "            data_train - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset train split, loaded as_supervided\n",
        "            data_valid - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split, loaded as_supervided\n",
        "            tokenizer_pt - Portuguese tokenizer created from the training set\n",
        "            tokenizer_en - English tokenizer created from the training set\n",
        "        \"\"\"\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        creates sub-word tokenizers for dataset\n",
        "        \n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The maximum vocab size should be set to 2**15\n",
        "        \n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n",
        "        \"\"\"\n",
        "        f = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
        "        en_tok = f((en.numpy() for _, en in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        pt_tok = f((pt.numpy() for pt, _ in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        return pt_tok, en_tok\n",
        "    \n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        encodes a translation into tokens\n",
        "\n",
        "        pt is the tf.Tensor containing the Portuguese sentence\n",
        "        en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The tokenized sentences should include the start and end of sentence tokens\n",
        "        The start token should be indexed as vocab_size\n",
        "        The end token should be indexed as vocab_size + 1\n",
        "        \n",
        "        Returns: pt_tokens, en_tokens\n",
        "            pt_tokens is a np.ndarray containing the Portuguese tokens\n",
        "            en_tokens is a np.ndarray. containing the English tokens\n",
        "        \"\"\"\n",
        "\n",
        "        return self.tokenizer_en.encode(en.numpy()), self.tokenizer_pt.encode(pt.numpy())\n"
      ],
      "metadata": {
        "id": "RF-VL0I2GLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(data.encode(pt, en))\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(data.encode(pt, en))\n",
        "\n",
        "# ([30138, 6, 36, 17925, 13, 3, 3037, 1, 4880, 3, 387, 2832, 18, 18444, 1, 5, 8, 3, 16679, 19460, 739, 2, 30139], \n",
        "# [28543, 4, 56, 15, 1266, 20397, 10721, 1, 15, 100, 125, 352, 3, 45, 3066, 6, 8004, 1, 88, 13, 14859, 2, 28544])\n",
        "# ([30138, 289, 15409, 2591, 19, 20318, 26024, 29997, 28, 30139], [28543, 93, 25, 907, 1366, 4, 5742, 33, 28544])"
      ],
      "metadata": {
        "id": "1yXdkNbdGMH_",
        "outputId": "b7960b14-fb91-44fa-9ccc-e5f2b5f925cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([98, 111, 101, 33, 120, 105, 102, 111, 33, 122, 112, 118, 33, 106, 110, 113, 115, 112, 119, 102, 33, 116, 102, 98, 115, 100, 105, 98, 99, 106, 109, 106, 117, 122, 33, 45, 33, 122, 112, 118, 33, 98, 100, 117, 118, 98, 109, 109, 122, 33, 117, 98, 108, 102, 33, 98, 120, 98, 122, 33, 117, 105, 102, 33, 112, 111, 102, 33, 98, 101, 119, 98, 111, 117, 98, 104, 102, 33, 112, 103, 33, 113, 115, 106, 111, 117, 33, 45, 33, 120, 105, 106, 100, 105, 33, 106, 116, 33, 116, 102, 115, 102, 111, 101, 106, 113, 106, 117, 122, 33, 47], [105, 36, 117, 121, 101, 114, 104, 115, 36, 113, 105, 112, 108, 115, 118, 101, 113, 115, 119, 36, 101, 36, 116, 118, 115, 103, 121, 118, 101, 36, 48, 36, 120, 109, 118, 101, 113, 115, 119, 36, 101, 36, 1, 114, 109, 103, 101, 36, 122, 101, 114, 120, 101, 107, 105, 113, 36, 104, 101, 36, 109, 113, 116, 118, 105, 119, 119, 3, 115, 36, 48, 36, 117, 121, 105, 36, 2, 36, 101, 36, 119, 105, 118, 105, 114, 104, 109, 116, 109, 104, 101, 104, 105, 36, 50])\n",
            "([101, 106, 101, 33, 117, 105, 102, 122, 33, 102, 98, 117, 33, 103, 106, 116, 105, 33, 98, 111, 101, 33, 100, 105, 106, 113, 116, 33, 64], [120, 109, 114, 108, 101, 113, 36, 103, 115, 113, 109, 104, 115, 36, 116, 105, 109, 124, 105, 36, 103, 115, 113, 36, 102, 101, 120, 101, 120, 101, 119, 36, 106, 118, 109, 120, 101, 119, 36, 67])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. TF Encode"
      ],
      "metadata": {
        "id": "Iawlxg6QGMY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        creates the instance attributes:\n",
        "            data_train - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset train split, loaded as_supervided\n",
        "            data_valid - contains the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split, loaded as_supervided\n",
        "            tokenizer_pt - Portuguese tokenizer created from the training set\n",
        "            tokenizer_en - English tokenizer created from the training set\n",
        "        \"\"\"\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "        self.data_train = self.data_train.map(self.tf_encode)\n",
        "        self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        creates sub-word tokenizers for dataset\n",
        "        \n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The maximum vocab size should be set to 2**15\n",
        "        \n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n",
        "        \"\"\"\n",
        "        f = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
        "        en_tok = f((en.numpy() for _, en in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        pt_tok = f((pt.numpy() for pt, _ in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        return pt_tok, en_tok\n",
        "    \n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        encodes a translation into tokens\n",
        "\n",
        "        pt is the tf.Tensor containing the Portuguese sentence\n",
        "        en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The tokenized sentences should include the start and end of sentence tokens\n",
        "        The start token should be indexed as vocab_size\n",
        "        The end token should be indexed as vocab_size + 1\n",
        "        \n",
        "        Returns: pt_tokens, en_tokens\n",
        "            pt_tokens is a np.ndarray containing the Portuguese tokens\n",
        "            en_tokens is a np.ndarray. containing the English tokens\n",
        "        \"\"\"\n",
        "\n",
        "        return self.tokenizer_en.encode(en.numpy()), self.tokenizer_pt.encode(pt.numpy())\n",
        "    \n",
        "    def tf_encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        acts as a tensorflow wrapper for the encode instance method\n",
        "        Make sure to set the shape of the pt and en return tensors\n",
        "        Update the class constructor def __init__(self):\n",
        "        update the data_train and data_validate attributes by tokenizing the examples\n",
        "        \"\"\"\n",
        "        return tf.py_function(self.encode, [pt, en], (tf.int32, tf.int32))\n"
      ],
      "metadata": {
        "id": "lx3xKtR8GORR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "print('got here')\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt, en)\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt, en)\n",
        "\n",
        "# tf.Tensor([30138     6    36 17925    13     3  3037     1  4880     3   387  2832   18    18444     1     5     8     3 16679 19460   739     2 30139], shape=(23,), dtype=int64) tf.Tensor([28543     4    56    15  1266 20397 10721     1    15   100   125   352  3    45  3066     6  8004     1    88    13 14859     2 28544], shape=(23,), dtype=int64)\n",
        "# tf.Tensor([30138   289 15409  2591    19 20318 26024 29997    28 30139], shape=(10,), dtype=int64) tf.Tensor([28543    93    25   907  1366     4  5742    33 28544], shape=(9,), dtype=int64)"
      ],
      "metadata": {
        "id": "OteM60nvGOU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d251d9-7019-4677-c284-60a638fb47de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got here\n",
            "tf.Tensor(\n",
            "[ 98 111 101  33 120 105 102 111  33 122 112 118  33 106 110 113 115 112\n",
            " 119 102  33 116 102  98 115 100 105  98  99 106 109 106 117 122  33  45\n",
            "  33 122 112 118  33  98 100 117 118  98 109 109 122  33 117  98 108 102\n",
            "  33  98 120  98 122  33 117 105 102  33 112 111 102  33  98 101 119  98\n",
            " 111 117  98 104 102  33 112 103  33 113 115 106 111 117  33  45  33 120\n",
            " 105 106 100 105  33 106 116  33 116 102 115 102 111 101 106 113 106 117\n",
            " 122  33  47], shape=(111,), dtype=int32) tf.Tensor(\n",
            "[105  36 117 121 101 114 104 115  36 113 105 112 108 115 118 101 113 115\n",
            " 119  36 101  36 116 118 115 103 121 118 101  36  48  36 120 109 118 101\n",
            " 113 115 119  36 101  36   1 114 109 103 101  36 122 101 114 120 101 107\n",
            " 105 113  36 104 101  36 109 113 116 118 105 119 119   3 115  36  48  36\n",
            " 117 121 105  36   2  36 101  36 119 105 118 105 114 104 109 116 109 104\n",
            " 101 104 105  36  50], shape=(95,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[101 106 101  33 117 105 102 122  33 102  98 117  33 103 106 116 105  33\n",
            "  98 111 101  33 100 105 106 113 116  33  64], shape=(29,), dtype=int32) tf.Tensor(\n",
            "[120 109 114 108 101 113  36 103 115 113 109 104 115  36 116 105 109 124\n",
            " 105  36 103 115 113  36 102 101 120 101 120 101 119  36 106 118 109 120\n",
            " 101 119  36  67], shape=(40,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pipeline"
      ],
      "metadata": {
        "id": "PPvXI3-gGOwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, max_len):\n",
        "        \"\"\"\n",
        "        creates the instance attributes:\n",
        "            data_train - contains the ted_hrlr_translate/pt_to_en encodings, loaded as_supervided\n",
        "            data_valid - contains the ted_hrlr_translate/pt_to_en encodings, loaded as_supervided\n",
        "            tokenizer_pt - Portuguese tokenizer created from the training set\n",
        "            tokenizer_en - English tokenizer created from the training set\n",
        "            batch_size is the batch size for training/validation\n",
        "            max_len is the maximum number of tokens allowed per example sentence\n",
        "\n",
        "update the data_train attribute by performing the following actions:\n",
        "    filter out all examples that have either sentence with more than max_len tokens\n",
        "    cache the dataset to increase performance\n",
        "    shuffle the entire dataset\n",
        "    split the dataset into padded batches of size batch_size\n",
        "    prefetch the dataset using tf.data.experimental.AUTOTUNE to increase performance\n",
        "update the data_validate attribute by performing the following actions:\n",
        "    filter out all examples that have either sentence with more than max_len tokens\n",
        "    split the dataset into padded batches of size batch_size\n",
        "        \"\"\"\n",
        "        self.max = max_len\n",
        "        self.bs = batch_size\n",
        "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
        "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en', split='validation', as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "        self.data_train = self.data_train.map(self.tf_encode)\n",
        "        self.data_train = self.data_train.filter(self.check_len)\n",
        "        self.data_train = self.data_train.cache()\n",
        "        self.data_train = self.data_train.shuffle(25000)\n",
        "        for i in self.data_train.take(1):\n",
        "          print(i)\n",
        "        self.data_train = self.data_train.padded_batch(batch_size, padded_shapes=(batch_size, -1))\n",
        "        self.data_train = self.data_train.prefetch(1000)\n",
        "\n",
        "        self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "        self.data_valid = self.data_valid.filter(self.check_len)\n",
        "        self.data_valid = self.data_valid.padded_batch(batch_size)\n",
        "\n",
        "    def check_len(self, a, b):\n",
        "        \"\"\"Checks for size == max_len\"\"\"\n",
        "        c = tf.logical_and(tf.size(a) <= self.max,\n",
        "                           tf.size(b) <= self.max)\n",
        "        return c\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        creates sub-word tokenizers for dataset\n",
        "        \n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The maximum vocab size should be set to 2**15\n",
        "        \n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n",
        "        \"\"\"\n",
        "        f = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
        "        en_tok = f((en.numpy() for _, en in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        pt_tok = f((pt.numpy() for pt, _ in data.take(10)),\n",
        "                         target_vocab_size=2**15)\n",
        "        return pt_tok, en_tok\n",
        "    \n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        encodes a translation into tokens\n",
        "\n",
        "        pt is the tf.Tensor containing the Portuguese sentence\n",
        "        en is the tf.Tensor containing the corresponding English sentence\n",
        "        \n",
        "        The tokenized sentences should include the start and end of sentence tokens\n",
        "        The start token should be indexed as vocab_size\n",
        "        The end token should be indexed as vocab_size + 1\n",
        "        \n",
        "        Returns: pt_tokens, en_tokens\n",
        "            pt_tokens is a np.ndarray containing the Portuguese tokens\n",
        "            en_tokens is a np.ndarray. containing the English tokens\n",
        "        \"\"\"\n",
        "        return self.tokenizer_en.encode(en.numpy()), self.tokenizer_pt.encode(pt.numpy())\n",
        "    \n",
        "    def tf_encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        acts as a tensorflow wrapper for the encode instance method\n",
        "        Make sure to set the shape of the pt and en return tensors\n",
        "        Update the class constructor def __init__(self):\n",
        "        update the data_train and data_validate attributes by tokenizing the examples\n",
        "        \"\"\"\n",
        "        return tf.py_function(self.encode, [pt, en], (tf.int32, tf.int32))"
      ],
      "metadata": {
        "id": "z3fCUk4TGRO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.set_random_seed(0)\n",
        "data = Dataset(32, 40)\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt, en)\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt, en)\n",
        "\n",
        "# tf.Tensor(\n",
        "# [[30138  1029   104 ...     0     0     0]\n",
        "#  [30138    40     8 ...     0     0     0]\n",
        "#  [30138    12    14 ...     0     0     0]\n",
        "#  ...\n",
        "#  [30138    72 23483 ...     0     0     0]\n",
        "#  [30138  2381   420 ...     0     0     0]\n",
        "#  [30138     7 14093 ...     0     0     0]], shape=(32, 39), dtype=int64) tf.Tensor(\n",
        "# [[28543   831   142 ...     0     0     0]\n",
        "#  [28543    16    13 ...     0     0     0]\n",
        "#  [28543    19     8 ...     0     0     0]\n",
        "#  ...\n",
        "#  [28543    18    27 ...     0     0     0]\n",
        "#  [28543  2648   114 ... 28544     0     0]\n",
        "#  [28543  9100 19214 ...     0     0     0]], shape=(32, 37), dtype=int64)\n",
        "# tf.Tensor(\n",
        "# [[30138   289 15409 ...     0     0     0]\n",
        "#  [30138    86   168 ...     0     0     0]\n",
        "#  [30138  5036     9 ...     0     0     0]\n",
        "#  ...\n",
        "#  [30138  1157 29927 ...     0     0     0]\n",
        "#  [30138    33   837 ...     0     0     0]\n",
        "#  [30138   126  3308 ...     0     0     0]], shape=(32, 32), dtype=int64) tf.Tensor(\n",
        "# [[28543    93    25 ...     0     0     0]\n",
        "#  [28543    11    20 ...     0     0     0]\n",
        "#  [28543    11  2850 ...     0     0     0]\n",
        "#  ...\n",
        "#  [28543    11   406 ...     0     0     0]\n",
        "#  [28543     9   152 ...     0     0     0]\n",
        "#  [28543     4   272 ...     0     0     0]], shape=(32, 35), dtype=int64)"
      ],
      "metadata": {
        "id": "KSqcqkJ7GQ9q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "acc04430-adfd-4528-ff62-a76a350f3c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
            "array([232, 220, 217, 231, 217, 148, 215, 220, 221, 224, 216, 230, 217,\n",
            "       226, 148, 225, 213, 232, 232, 217, 230,   2], dtype=int32)>, <tf.Tensor: shape=(25,), dtype=int32, numpy=\n",
            "array([215, 229, 229, 101, 213, 228, 219, 211, 224, 309, 281, 101,  32,\n",
            "       219, 223, 226, 225, 228, 230, 211, 224, 230, 215, 229,   4],\n",
            "      dtype=int32)>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    786\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mdims_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_padded_shape_to_tensor\u001b[0;34m(padded_shape, input_component_shape)\u001b[0m\n\u001b[1;32m   5009\u001b[0m     \u001b[0;31m# Try to convert the `padded_shape` to a `tf.TensorShape`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5010\u001b[0;31m     \u001b[0mpadded_shape_as_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5011\u001b[0m     \u001b[0;31m# We will return the \"canonical\" tensor representation, which uses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Treat as a singleton dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dimension %d must be >= 0\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimension -1 must be >= 0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6722487dc328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-0673d6d160ba>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, max_len)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ted_hrlr_translate/pt_to_en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ted_hrlr_translate/pt_to_en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mpadded_batch\u001b[0;34m(self, batch_size, padded_shapes, padding_values, drop_remainder, name)\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mpadding_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m   def map(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, batch_size, padded_shapes, padding_values, drop_remainder, name)\u001b[0m\n\u001b[1;32m   5124\u001b[0m         nest.flatten(input_shapes), flat_padded_shapes):\n\u001b[1;32m   5125\u001b[0m       flat_padded_shapes_as_tensors.append(\n\u001b[0;32m-> 5126\u001b[0;31m           _padded_shape_to_tensor(padded_shape, input_component_shape))\n\u001b[0m\u001b[1;32m   5127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5128\u001b[0m     self._padded_shapes = nest.pack_sequence_as(input_shapes,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_padded_shape_to_tensor\u001b[0;34m(padded_shape, input_component_shape)\u001b[0m\n\u001b[1;32m   5022\u001b[0m       six.reraise(ValueError, ValueError(\n\u001b[1;32m   5023\u001b[0m           \u001b[0;34mf\"Padded shape {padded_shape} must be a `tf.int64` vector tensor, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5024\u001b[0;31m           f\"but its shape was {ret.shape}.\"), sys.exc_info()[2])\n\u001b[0m\u001b[1;32m   5025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5026\u001b[0m       six.reraise(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_padded_shape_to_tensor\u001b[0;34m(padded_shape, input_component_shape)\u001b[0m\n\u001b[1;32m   5008\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5009\u001b[0m     \u001b[0;31m# Try to convert the `padded_shape` to a `tf.TensorShape`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5010\u001b[0;31m     \u001b[0mpadded_shape_as_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5011\u001b[0m     \u001b[0;31m# We will return the \"canonical\" tensor representation, which uses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5012\u001b[0m     \u001b[0;31m# `-1` in place of `None`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Treat as a singleton dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dimension %d must be >= 0\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Padded shape -1 must be a `tf.int64` vector tensor, but its shape was ()."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Masks"
      ],
      "metadata": {
        "id": "juyrMsOyGOqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inputs, target):\n",
        "    \"\"\"\n",
        "    creates all masks for training/validation:\n",
        "\n",
        "    inputs is a tf.Tensor of shape (batch_size, seq_len_in) that contains the input sentence\n",
        "    target is a tf.Tensor of shape (batch_size, seq_len_out) that contains the target sentence\n",
        "    \n",
        "    uses only tensorflow operations in order to properly function in the training step\n",
        "    \n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "        encoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1, seq_len_in) to be applied in the encoder\n",
        "        combined_mask is the tf.Tensor of shape (batch_size, 1, seq_len_out, seq_len_out) used in the 1st attention block \n",
        "            in the decoder to pad and mask future tokens in the input received by the decoder. \n",
        "            It takes the maximum between a lookaheadmask and the decoder target padding mask.\n",
        "        decoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1, seq_len_in) used in the 2nd attention block in the decoder.\n",
        "    \"\"\"\n",
        "    enc_mask = padding_mask(inputs)\n",
        "    dec_mask = padding_mask(inputs)\n",
        "    look_ahead = look_ahead_mask(tf.shape(target)[1])\n",
        "    tar_mask = padding_mask(target)\n",
        "    combined_mask = tf.maximum(tar_mask, look_ahead)\n",
        "    return enc_mask, combined_mask, dec_mask\n",
        "\n",
        "def padding_mask(seq):\n",
        "    \"\"\"Creates padded mask\"\"\"\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def look_ahead_mask(size):\n",
        "    \"\"\"Creates look ahead mask\"\"\"\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask[tf.newaxis, ...]\n"
      ],
      "metadata": {
        "id": "1q6Um9LuGTl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXrqUsK9GTXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train"
      ],
      "metadata": {
        "id": "7bEhci9fGT5w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSJJdIBSGWM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUiNQIm0GWa8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}